{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28cfc569",
   "metadata": {},
   "source": [
    "ü™ê The Search for Planet 9: Complete Computational Pipeline\n",
    "Project Overview: This notebook synthesizes the search for \"Planet 9\" (and the alternative \"Planet Y\") using a chronological pipeline of 30+ scripts. Method: We start by harvesting live data, then move to Statistical Clustering (Phase 1), Machine Learning Classification (Phase 2), Bias De-biasing (Phase 3), and finally N-Body Physics (Phase 4).\n",
    "\n",
    "üõ† Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries if missing\n",
    "!pip install numpy pandas matplotlib scipy scikit-learn tensorflow seaborn requests rebound"
   ]
  }, 
{
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba86ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys",
    "# Code ran on VCS as a JupyterNotebook. If it gives errors when downloading tensorflow, use the following:\n",
    "!{sys.executable} -m pip install --upgrade tensorflow\n",
    "It it gives errors when using the module \n",
    "!{sys.executable} -m pip install --upgrade scikit-learn"
   ]
  }
  {
   "cell_type": "markdown",
   "id": "ae3b6070",
   "metadata": {},
   "source": [
    "üì° Phase 1: Data Harvesting & Preparation\n",
    "Script 01: The Data Harvester (Reconstructed)\n",
    "Goal: Download the latest Trans-Neptunian Object (TNO) data directly from the Minor Planet Center (MPC), filter for \"Extreme\" objects (ETNOs), and feature-engineer the angular components needed for the AI models. Output: processed_etnos.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65210afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MPC_URL = \"https://www.minorplanetcenter.net/iau/MPCORB/Distant.txt\"\n",
    "OUTPUT_FILE = \"processed_etnos.csv\"\n",
    "\n",
    "print(f\"Downloading live data from {MPC_URL}...\")\n",
    "\n",
    "def fetch_and_process_data():\n",
    "    # 1. Fetch Raw Data\n",
    "    try:\n",
    "        response = requests.get(MPC_URL)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"Download Failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Parse Fixed-Width MPC Format\n",
    "    # The MPC format is messy fixed-width text. We parse it line by line.\n",
    "    data = []\n",
    "    lines = response.text.split('\\n')\n",
    "    \n",
    "    print(f\"Parsing {len(lines)} objects...\")\n",
    "    \n",
    "    for line in lines:\n",
    "        if len(line) < 100: continue # Skip headers/empty lines\n",
    "        try:\n",
    "            # Extract orbital elements (Fixed width positions)\n",
    "            # a (Semi-major axis) is not explicit, we get 'n' (mean motion) or 'a' directly if available\n",
    "            # Standard MPC Distant format usually has 'a' in columns roughly 92-103 or derived from 'n'\n",
    "            # Let's try standard MPCORB unpacking:\n",
    "            \n",
    "            # This parser assumes standard MPC 1-line format\n",
    "            # We use a robust method: split by whitespace if fixed-width fails, \n",
    "            # but for MPC, fixed width is safer.\n",
    "            \n",
    "            # H_mag = line[8:13]\n",
    "            # Epoch = line[20:25]\n",
    "            # M = line[26:35]\n",
    "            # peri (w) = line[37:46]\n",
    "            # node (Node) = line[48:57]\n",
    "            # inc (i) = line[59:68]\n",
    "            # e = line[70:79]\n",
    "            # n (mean daily motion) = line[80:91]\n",
    "            # a = line[92:103]\n",
    "            \n",
    "            w = float(line[37:46])\n",
    "            node = float(line[48:57])\n",
    "            i = float(line[59:68])\n",
    "            e = float(line[70:79])\n",
    "            \n",
    "            # Calculate 'a' from 'n' if 'a' is missing, or read 'a' directly\n",
    "            try:\n",
    "                a = float(line[92:103])\n",
    "            except:\n",
    "                # Fallback: Kepler's 3rd Law: a = (0.9856 / n)^(2/3)\n",
    "                n = float(line[80:91])\n",
    "                if n > 0:\n",
    "                    a = (0.9856 / n) ** (2/3)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Calculate Perihelion (q)\n",
    "            q = a * (1 - e)\n",
    "            \n",
    "            # FILTER: We want \"Extreme\" TNOs (ETNOs)\n",
    "            # Standard Definition: a > 150 AU and q > 30 AU\n",
    "            if a > 230 and q > 30:\n",
    "                # Calculate Longitude of Perihelion (varpi)\n",
    "                # varpi = w + Node\n",
    "                varpi = (w + node) % 360\n",
    "                \n",
    "                data.append({\n",
    "                    'a': a, 'q': q, 'e': e, 'i': i, \n",
    "                    'w': w, 'Node': node, 'varpi': varpi\n",
    "                })\n",
    "                \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # 3. Feature Engineering for AI\n",
    "    # ML models handle sin/cos better than raw degrees (avoids 0/360 wrap-around issue)\n",
    "    df['varpi_rad'] = np.radians(df['varpi'])\n",
    "    df['i_rad'] = np.radians(df['i'])\n",
    "    df['varpi_sin'] = np.sin(df['varpi_rad'])\n",
    "    df['varpi_cos'] = np.cos(df['varpi_rad'])\n",
    "    \n",
    "    print(f\"Success! Extracted {len(df)} Extreme TNOs.\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Save to disk\n",
    "    df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"Database saved to {OUTPUT_FILE}\")\n",
    "\n",
    "# Run the Harvester\n",
    "fetch_and_process_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd9ab8d",
   "metadata": {},
   "source": [
    "üß† Phase 2: The AI \"Gut Check\"\n",
    "Script 02: Autoencoder Model\n",
    "Goal: To determine if the orbital clustering is \"anomalous\" compared to random noise. How it works: We train a Neural Network (Autoencoder) on random solar systems. We then feed it the Real solar system. If the error is high, the real solar system is \"weird\" (structured). Interpretation: A high \"anomaly score\" suggests external forces (like Planet 9) are sculpting the orbits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a59cae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "FEATURE_COLS = ['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos'] \n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"CRITICAL ERROR: {INPUT_FILE} missing. Run Script 01 first.\")\n",
    "else:\n",
    "    df_real = pd.read_csv(INPUT_FILE)\n",
    "    print(f\"Loaded {len(df_real)} High-Quality ETNOs.\")\n",
    "\n",
    "    # 1. Generate Synthetic Data (The \"Null Hypothesis\")\n",
    "    # We scramble the real data to create a \"Random Universe\"\n",
    "    def generate_synthetic_data(n_samples=5000, real_df=None):\n",
    "        syn_a = np.random.choice(real_df['a'], n_samples, replace=True)\n",
    "        syn_e = np.random.choice(real_df['e'], n_samples, replace=True)\n",
    "        syn_i = np.random.choice(real_df['i_rad'], n_samples, replace=True)\n",
    "        \n",
    "        # Randomize angles (This removes any Planet 9 clustering)\n",
    "        syn_varpi = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "        \n",
    "        syn_data = pd.DataFrame({\n",
    "            'a': syn_a, 'e': syn_e, 'i_rad': syn_i,\n",
    "            'varpi_sin': np.sin(syn_varpi), 'varpi_cos': np.cos(syn_varpi)\n",
    "        })\n",
    "        return syn_data\n",
    "\n",
    "    print(\"Generating Synthetic Data (Null Hypothesis)...\")\n",
    "    df_synthetic = generate_synthetic_data(n_samples=5000, real_df=df_real)\n",
    "\n",
    "    # 2. Preprocessing\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(df_synthetic[FEATURE_COLS])\n",
    "    X_test = scaler.transform(df_real[FEATURE_COLS])\n",
    "\n",
    "    # 3. Build the Autoencoder\n",
    "    # It compresses data into 2 dimensions. If data is random, compression is hard.\n",
    "    # If data is clustered (Planet 9), compression is easy (or distinct).\n",
    "    input_dim = len(FEATURE_COLS)\n",
    "    encoding_dim = 2 \n",
    "\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoder = layers.Dense(12, activation=\"relu\")(input_layer)\n",
    "    bottleneck = layers.Dense(encoding_dim, activation=\"linear\")(encoder) \n",
    "    decoder = layers.Dense(12, activation=\"relu\")(bottleneck)\n",
    "    output_layer = layers.Dense(input_dim, activation=\"sigmoid\")(decoder)\n",
    "\n",
    "    autoencoder = models.Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # 4. Train the AI Judge\n",
    "    print(\"Training AI Judge...\")\n",
    "    history = autoencoder.fit(X_train, X_train, epochs=50, batch_size=16, shuffle=True, verbose=0)\n",
    "\n",
    "    # 5. Analysis\n",
    "    reconstructions = autoencoder.predict(X_test, verbose=0)\n",
    "    mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "    df_real['anomaly_score'] = mse\n",
    "\n",
    "    print(\"\\n--- RESULTS ---\")\n",
    "    print(\"Top 3 Candidates for Planet 9 Interaction (Highest Anomaly Score):\")\n",
    "    print(df_real.sort_values('anomaly_score', ascending=False)[['a', 'varpi', 'anomaly_score']].head(3))\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.hist(df_real['anomaly_score'], bins=10, alpha=0.7, color='blue', label='Real ETNOs', density=True)\n",
    "    plt.title(\"Reconstruction Error: Are Real Orbits Weird?\")\n",
    "    plt.xlabel(\"Anomaly Score (Higher = More structured/Weird)\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1251b68d",
   "metadata": {},
   "source": [
    "üìä Phase 3: Visual & Statistical Proofs\n",
    "Script 03: The Rose Diagram\n",
    "Goal: To visually confirm the clustering of the \"Longitude of Perihelion\" (the angle the orbits point). Interpretation: Planet 9 is predicted to be at ~241¬∞. The objects should cluster \"anti-aligned\" at ~61¬∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(\"Error: processed_etnos.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Filter for the High-Quality Objects only (matching your Step 2 filter)\n",
    "# Adjust 'a' cutoff if you changed it in step 01 (e.g. 150 or 230)\n",
    "df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "\n",
    "print(f\"Plotting {len(df_hq)} High-Quality Objects...\")\n",
    "\n",
    "# --- PLOT: The Rose Diagram ---\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# 1. Plot the Histogram (The Fan)\n",
    "# We want to see if the angles cluster around 60 degrees\n",
    "num_bins = 24\n",
    "counts, bin_edges = np.histogram(df_hq['varpi_rad'], bins=num_bins, range=(0, 2*np.pi))\n",
    "widths = np.diff(bin_edges)\n",
    "\n",
    "# Color the bars based on count (Darker = More Clustering)\n",
    "bars = ax.bar(bin_edges[:-1], counts, width=widths, bottom=0.0, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# 2. Add the \"Planet 9 Prediction\" line\n",
    "# Planet 9 is suspected to be at varpi ~241 degrees.\n",
    "# The cluster should be Anti-Aligned (~61 degrees).\n",
    "p9_angle = np.radians(241)\n",
    "cluster_angle = np.radians(61)\n",
    "\n",
    "ax.axvline(cluster_angle, color='red', linewidth=3, linestyle='--', label='Predicted Cluster (Anti-P9)')\n",
    "ax.axvline(p9_angle, color='green', linewidth=3, linestyle='--', label='Planet 9 Location')\n",
    "\n",
    "# 3. Highlight your Top 3 Anomalies (From your previous result)\n",
    "# Object 13 (110 deg), Object 3 (59 deg), Object 9 (25 deg)\n",
    "anomalies = [np.radians(110), np.radians(59), np.radians(25)]\n",
    "ax.scatter(anomalies, [max(counts)]*3, c='yellow', s=150, edgecolors='black', zorder=10, label='ML Top Anomalies')\n",
    "\n",
    "# Styling\n",
    "ax.set_theta_zero_location(\"E\") # Set 0 degrees to East\n",
    "ax.set_theta_direction(1)       # Counter-clockwise\n",
    "ax.set_title(\"Longitude of Perihelion Clustering\\n(The 'Smoking Gun' for Planet 9)\", va='bottom', fontsize=14)\n",
    "ax.legend(loc='lower left', bbox_to_anchor=(-0.1, -0.1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf9f84",
   "metadata": {},
   "source": [
    "Script 04: Statistical Validation (K-S Test)\n",
    "\n",
    "The Logic:Visual clustering (like the Rose Diagram) can be subjective. We need a hard number. This script applies the Kolmogorov-Smirnov (K-S) Test, a non-parametric test that compares our data's cumulative distribution against a perfectly uniform (random) distribution.\n",
    "\n",
    "Why this exists:To provide a \"p-value\" for the clustering.\n",
    "\n",
    "Null Hypothesis: The angles are random (uniformly distributed).Alternative: The angles are clustered (drawn from a non-uniform distribution).The Threshold: If $p < 0.05$, we reject the random model with 95% confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78353485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import kstest, uniform\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "# script_dir = os.path.dirname(os.path.abspath(__file__)) # Commented out for notebook\n",
    "# input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "input_path = INPUT_FILE\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(\"Error: processed_etnos.csv not found. Run Script 01.\")\n",
    "else:\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    # 1. Filter High-Quality Objects\n",
    "    # (Using the same strict cutoffs as before)\n",
    "    df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "    print(f\"Analyzing Statistical Significance of {len(df_hq)} Objects...\")\n",
    "\n",
    "    # 2. Prepare Data\n",
    "    # Normalize angles to 0-1 range for the KS test (0 = 0 deg, 1 = 360 deg)\n",
    "    # We use varpi (Longitude of Perihelion)\n",
    "    angles_normalized = df_hq['varpi'] / 360.0\n",
    "\n",
    "    # 3. The Kolmogorov-Smirnov (KS) Test\n",
    "    # Null Hypothesis: The angles are uniformly distributed (Random).\n",
    "    # Alternative: They are NOT uniform (Clustered).\n",
    "    statistic, p_value = kstest(angles_normalized, 'uniform')\n",
    "\n",
    "    print(\"\\n--- STATISTICAL VERDICT ---\")\n",
    "    print(f\"KS Statistic: {statistic:.4f} (How 'bunchy' the data is)\")\n",
    "    print(f\"P-Value:      {p_value:.5f}\")\n",
    "\n",
    "    print(\"\\n--- INTERPRETATION ---\")\n",
    "    if p_value < 0.01:\n",
    "        print(\"RESULT: HIGHLY SIGNIFICANT CLUSTERING (99%+ Confidence)\")\n",
    "        print(\"This is publishable evidence. The random model is rejected.\")\n",
    "    elif p_value < 0.05:\n",
    "        print(\"RESULT: Significant Clustering (95% Confidence)\")\n",
    "        print(\"Strong signal, likely real.\")\n",
    "    else:\n",
    "        print(\"RESULT: Indistinguishable from Randomness.\")\n",
    "        print(\"Current sample size may be too small or noise is too high.\")\n",
    "\n",
    "    # 4. Visual Cumulative Distribution (CDF) Plot\n",
    "    # If random, the blue line should follow the diagonal dashed line.\n",
    "    # If clustered, the blue line will look like 'stairs' or curve away.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(np.sort(angles_normalized), np.linspace(0, 1, len(angles_normalized), endpoint=False), label='Your Data (CDF)')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random Uniform (Expected)')\n",
    "    plt.xlabel(\"Normalized Angle (0 to 360 deg)\")\n",
    "    plt.ylabel(\"Cumulative Probability\")\n",
    "    plt.title(f\"KS Test for Clustering (p={p_value:.4f})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db30718a",
   "metadata": {},
   "source": [
    "Script 05: Circular Statistics (Rayleigh Test)\n",
    "Goal: Mathematically prove the cluster isn't just a coincidence. Method: Uses the Rayleigh Test (calculating the vector sum of angles) and a Monte Carlo Simulation (running 100,000 random universes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2124d900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(\"Error: processed_etnos.csv not found.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# 1. Filter High-Quality Objects\n",
    "# Use the strict filter (a > 230) to remove Neptune noise\n",
    "df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "angles_rad = np.radians(df_hq['varpi']) # Convert to radians\n",
    "n_objects = len(df_hq)\n",
    "\n",
    "print(f\"Analyzing {n_objects} High-Quality Objects...\")\n",
    "\n",
    "# --- TEST 1: The Rayleigh Test (Vector Sum) ---\n",
    "# Calculate the \"Mean Resultant Length\" (R_bar)\n",
    "# If R_bar is close to 1, they are pointing in the same direction.\n",
    "# If R_bar is close to 0, they are random.\n",
    "C = np.sum(np.cos(angles_rad))\n",
    "S = np.sum(np.sin(angles_rad))\n",
    "R = np.sqrt(C**2 + S**2)\n",
    "R_bar = R / n_objects\n",
    "\n",
    "# Calculate P-value using the Rayleigh approximation\n",
    "# Z = n * R_bar^2\n",
    "Z = n_objects * (R_bar**2)\n",
    "p_value_rayleigh = np.exp(-Z) * (1 + (2*Z - Z**2)/(4*n_objects) - (24*Z - 132*Z**2 + 76*Z**3 - 9*Z**4)/(288*n_objects**2))\n",
    "\n",
    "print(\"\\n--- RAYLEIGH TEST RESULTS ---\")\n",
    "print(f\"Mean Vector Length (0-1): {R_bar:.4f}\")\n",
    "print(f\"Z-Statistic:              {Z:.4f}\")\n",
    "print(f\"P-Value (Analytical):     {p_value_rayleigh:.5f}\")\n",
    "\n",
    "\n",
    "# --- TEST 2: Monte Carlo Simulation (The \"Gold Standard\") ---\n",
    "# We generate 100,000 random sets of N objects and see how often they cluster this well.\n",
    "print(\"\\n--- RUNNING MONTE CARLO SIMULATION (100k Runs) ---\")\n",
    "n_simulations = 100000\n",
    "random_R_bars = []\n",
    "\n",
    "# Generate random angles for all simulations at once for speed\n",
    "random_angles = np.random.uniform(0, 2*np.pi, (n_simulations, n_objects))\n",
    "# Calculate vector sums for all simulations\n",
    "sim_C = np.sum(np.cos(random_angles), axis=1)\n",
    "sim_S = np.sum(np.sin(random_angles), axis=1)\n",
    "sim_R = np.sqrt(sim_C**2 + sim_S**2)\n",
    "sim_R_bars = sim_R / n_objects\n",
    "\n",
    "# Count how many random simulations beat our real score\n",
    "better_sims = np.sum(sim_R_bars >= R_bar)\n",
    "p_value_mc = better_sims / n_simulations\n",
    "\n",
    "print(f\"Simulations that beat your data: {better_sims} out of {n_simulations}\")\n",
    "print(f\"Monte Carlo P-Value: {p_value_mc:.5f}\")\n",
    "\n",
    "# --- INTERPRETATION ---\n",
    "print(\"\\n--- FINAL VERDICT ---\")\n",
    "if p_value_mc < 0.01:\n",
    "    print(\"STATUS: CONFIRMED. (99%+ Confidence)\")\n",
    "    print(\"The clustering is statistically real.\")\n",
    "elif p_value_mc < 0.05:\n",
    "    print(\"STATUS: PROMISING. (95% Confidence)\")\n",
    "    print(\"Likely real, but more data would help.\")\n",
    "else:\n",
    "    print(\"STATUS: RANDOM.\")\n",
    "    print(f\"There is a {p_value_mc*100:.1f}% chance this is just noise.\")\n",
    "\n",
    "# --- PLOT: Visual Proof ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sim_R_bars, bins=50, color='gray', alpha=0.5, label='Random Chance (Monte Carlo)')\n",
    "plt.axvline(R_bar, color='red', linestyle='dashed', linewidth=2, label=f'Your Data (R={R_bar:.2f})')\n",
    "plt.xlabel(\"Vector Clustering Strength (R_bar)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Is your cluster real? (P = {p_value_mc:.5f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3169970",
   "metadata": {},
   "source": [
    "Script 06: Subset Analysis (The \"Top N\" Check)\n",
    "The Logic: Critics often argue: \"If you pick the 10 weirdest objects, of course they look clustered.\" This script tests that specific critique. It uses the Autoencoder to identify the \"Top 12 Anomalies\" but validates them against a Monte Carlo simulation. Why this exists: To ensure we aren't fooling ourselves with selection bias.\n",
    "\n",
    "The Test: We simulate 2,000 random universes. In each one, we pick the \"Top 12 Weirdest\" objects (highest reconstruction error).\n",
    "\n",
    "The Comparison: Do our Top 12 cluster more tightly than the Top 12 from a random universe? If yes, our anomalies are special. If no, our \"signal\" is just a mathematical artifact of picking outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b594154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "TOP_N = 12  # We will test the \"Top 12\" weirdest objects\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: processed_etnos.csv not found.\")\n",
    "else:\n",
    "    # 1. Load & Filter Data\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "    print(f\"Starting with {len(df_hq)} objects. hunting for the Top {TOP_N} anomalies...\")\n",
    "\n",
    "    # 2. Build & Train Autoencoder (Fast version)\n",
    "    feature_cols = ['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos'] \n",
    "\n",
    "    # Synthetic Data Gen\n",
    "    n_syn = 30000\n",
    "    syn_a = np.random.choice(df_hq['a'], n_syn, replace=True)\n",
    "    syn_e = np.random.choice(df_hq['e'], n_syn, replace=True)\n",
    "    syn_i = np.random.choice(df_hq['i_rad'], n_syn, replace=True)\n",
    "    syn_varpi = np.random.uniform(0, 2*np.pi, n_syn)\n",
    "    df_syn = pd.DataFrame({'a':syn_a, 'e':syn_e, 'i_rad':syn_i, \n",
    "                           'varpi_sin':np.sin(syn_varpi), 'varpi_cos':np.cos(syn_varpi)})\n",
    "\n",
    "    # Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(df_syn[feature_cols])\n",
    "    X_test = scaler.transform(df_hq[feature_cols])\n",
    "\n",
    "    # Model\n",
    "    input_dim = 5\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    enc = layers.Dense(12, activation=\"relu\")(input_layer)\n",
    "    bot = layers.Dense(2, activation=\"linear\")(enc)\n",
    "    dec = layers.Dense(12, activation=\"relu\")(bot)\n",
    "    out = layers.Dense(input_dim, activation=\"sigmoid\")(dec)\n",
    "    ae = models.Model(input_layer, out)\n",
    "    ae.compile(optimizer='adam', loss='mse')\n",
    "    ae.fit(X_train, X_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # 3. Identify the \"Chosen Ones\"\n",
    "    reconstructions = ae.predict(X_test, verbose=0)\n",
    "    mse = np.mean(np.power(X_test - reconstructions, 2), axis=1)\n",
    "    df_hq['anomaly_score'] = mse\n",
    "\n",
    "    # Sort and take the Top N\n",
    "    df_top = df_hq.sort_values('anomaly_score', ascending=False).head(TOP_N).copy()\n",
    "    print(f\"\\n--- TOP {TOP_N} ANOMALIES ---\")\n",
    "    print(df_top[['a', 'varpi', 'anomaly_score']])\n",
    "\n",
    "    # 4. Run Stats on the Top N\n",
    "    angles_rad = np.radians(df_top['varpi'])\n",
    "    C = np.sum(np.cos(angles_rad))\n",
    "    S = np.sum(np.sin(angles_rad))\n",
    "    R_bar = np.sqrt(C**2 + S**2) / TOP_N\n",
    "\n",
    "    print(f\"\\nMean Vector Length (R_bar): {R_bar:.4f}\")\n",
    "\n",
    "    # 5. Monte Carlo Validation (The \"Cherry Picking\" Check)\n",
    "    print(\"\\nRunning Validation (Is this just a math trick?)...\")\n",
    "    n_sims = 2000\n",
    "    beats = 0\n",
    "\n",
    "    for i in range(n_sims):\n",
    "        # Create a FAKE dataset (Random angles)\n",
    "        fake_varpi = np.random.uniform(0, 2*np.pi, len(df_hq))\n",
    "        fake_data = df_hq.copy()\n",
    "        fake_data['varpi_sin'] = np.sin(fake_varpi)\n",
    "        fake_data['varpi_cos'] = np.cos(fake_varpi)\n",
    "        \n",
    "        # Scale\n",
    "        X_fake = scaler.transform(fake_data[feature_cols])\n",
    "        \n",
    "        # Get Error Scores (using the SAME model)\n",
    "        fake_recon = ae.predict(X_fake, verbose=0)\n",
    "        fake_mse = np.mean(np.power(X_fake - fake_recon, 2), axis=1)\n",
    "        \n",
    "        # Pick Top N of this fake batch\n",
    "        fake_indices = np.argsort(fake_mse)[-TOP_N:] # Indices of top N errors\n",
    "        top_fake_angles = fake_varpi[fake_indices]\n",
    "        \n",
    "        # Measure Clustering\n",
    "        f_C = np.sum(np.cos(top_fake_angles))\n",
    "        f_S = np.sum(np.sin(top_fake_angles))\n",
    "        f_R = np.sqrt(f_C**2 + f_S**2) / TOP_N\n",
    "        \n",
    "        if f_R >= R_bar:\n",
    "            beats += 1\n",
    "\n",
    "    p_value = beats / n_sims\n",
    "\n",
    "    print(\"\\n--- FINAL VERDICT (SUBSET ANALYSIS) ---\")\n",
    "    print(f\"P-Value: {p_value:.5f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"SUCCESS: The 'Anomalies' are significantly clustered.\")\n",
    "    else:\n",
    "        print(\"FAILURE: Even the anomalies are random.\")\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    ax.scatter(angles_rad, [1]*TOP_N, c='blue', s=100, label=f'Top {TOP_N} Anomalies')\n",
    "    ax.vlines(np.radians(61), 0, 1, color='red', linestyle='--', label='P9 Prediction')\n",
    "    ax.set_title(f\"Top {TOP_N} Anomalies (p={p_value:.3f})\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1369d9b6",
   "metadata": {},
   "source": [
    "Script 07: The \"Bimodal\" Camps Visualizer\n",
    "The Logic: Statistical clustering is often abstract. This script bridges the gap between numbers and geometry. If a massive perturber is shepherding these objects, they shouldn't just be \"clustered\"; they should fall into two specific dynamical populations:\n",
    "\n",
    "Camp A (Aligned): Objects physically clustered with the planet (metastable).\n",
    "\n",
    "Camp B (Anti-Aligned): Objects clustered 180¬∞ away from the planet (stable). Why this exists: We need to verify if the clustering follows this specific Bimodal Physics signature. If we see a \"smear\" rather than two distinct lobes, the shepherding hypothesis weakens. This script isolates specific objects (by ID) to see which \"Camp\" they belong to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cde8a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "df = pd.read_csv(input_path)\n",
    "\n",
    "# Filter High Quality\n",
    "df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "\n",
    "# Recover the anomalies from your logs (Hardcoded for visualization based on your data)\n",
    "# We highlight the two camps\n",
    "camp_a = [25, 38, 59, 61, 110, 122] # The \"Planet 9\" side\n",
    "camp_b = [196, 218, 270, 305, 306, 317] # The \"Opposite\" side\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "\n",
    "# 1. Plot All High Quality Objects (Grey background)\n",
    "angles_all = np.radians(df_hq['varpi'])\n",
    "ax.scatter(angles_all, [1]*len(angles_all), c='gray', alpha=0.3, s=50, label='Background Noise')\n",
    "\n",
    "# 2. Plot Camp A (Red - Matches P9)\n",
    "ax.scatter(np.radians(camp_a), [1.1]*len(camp_a), c='red', s=150, edgecolors='black', label='Camp A: Aligned w/ Prediction')\n",
    "\n",
    "# 3. Plot Camp B (Blue - Contradicts P9)\n",
    "ax.scatter(np.radians(camp_b), [1.1]*len(camp_b), c='blue', s=150, edgecolors='black', label='Camp B: Anti-Aligned')\n",
    "\n",
    "# 4. Annotations\n",
    "ax.axvline(np.radians(61), color='red', linestyle='--', alpha=0.5, label='Planet 9 Prediction')\n",
    "ax.set_title(\"The 'Bimodal' Problem:\\nML Finds Stable Objects on BOTH Sides\", va='bottom', fontsize=14)\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.5, -0.15))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5491a",
   "metadata": {},
   "source": [
    "Script 08: Galactic Plane & Bias Correlation\n",
    "The Logic: The \"Elephant in the Room\" for any TNO search is the Milky Way. We cannot detect faint objects against the dense star field of the galaxy. Why this exists: Critics argue that we only find clustering because we can't look everywhere (the \"Look-Elsewhere Effect\"). This script overlays our object data on top of the \"Galactic Avoidance Zones\" (where surveys are blind).\n",
    "\n",
    "Hypothesis Check: If the cluster aligns perfectly with the edges of the Milky Way, our data is likely just observational bias. If the cluster exists independent of these gaps, the signal is real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244ef89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "df_hq = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "\n",
    "# Recover your Top 12 Anomalies (Camp A & B)\n",
    "# We need their specific angles (varpi) and their \"Node\" (Longitude of Ascending Node)\n",
    "# to see where they are in the sky relative to the Galaxy.\n",
    "# (Note: We are approximating survey bias using Galactic Latitude exclusion zones here)\n",
    "\n",
    "# Re-run the quick scorer to get the top 12 indices\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "feat_cols = ['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']\n",
    "\n",
    "# Quick synthetic train (just to get scores back)\n",
    "# Ideally you'd save scores in previous step, but this is fast:\n",
    "syn_varpi = np.random.uniform(0, 2*np.pi, 5000)\n",
    "X_syn = scaler.fit_transform(pd.DataFrame({\n",
    "    'a': np.random.choice(df_hq['a'], 5000),\n",
    "    'e': np.random.choice(df_hq['e'], 5000),\n",
    "    'i_rad': np.random.choice(df_hq['i_rad'], 5000),\n",
    "    'varpi_sin': np.sin(syn_varpi), 'varpi_cos': np.cos(syn_varpi)\n",
    "}))\n",
    "X_real = scaler.transform(df_hq[feat_cols])\n",
    "\n",
    "# Simple Autoencoder (Replicating your architecture)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "inp = layers.Input(shape=(5,))\n",
    "e = layers.Dense(12, activation='relu')(inp)\n",
    "b = layers.Dense(2, activation='linear')(e)\n",
    "d = layers.Dense(12, activation='relu')(b)\n",
    "out = layers.Dense(5, activation='sigmoid')(d)\n",
    "m = models.Model(inp, out)\n",
    "m.compile(loss='mse', optimizer='adam')\n",
    "m.fit(X_syn, X_syn, epochs=30, verbose=0)\n",
    "\n",
    "# Get Scores\n",
    "recon = m.predict(X_real, verbose=0)\n",
    "df_hq['score'] = np.mean(np.power(X_real - recon, 2), axis=1)\n",
    "top_12 = df_hq.sort_values('score', ascending=False).head(12)\n",
    "\n",
    "# --- THE \"KILL SHOT\" PLOT ---\n",
    "# We plot the Longitude of Perihelion (varpi) vs Longitude of Ascending Node (Node)\n",
    "# Survey biases often show up as correlations here.\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# 1. Plot all background objects\n",
    "plt.scatter(df_hq['Node'], df_hq['varpi'], c='gray', alpha=0.3, label='Background Objects')\n",
    "\n",
    "# 2. Plot the \"Anomalies\"\n",
    "plt.scatter(top_12['Node'], top_12['varpi'], c='red', s=100, edgecolors='black', label='Top 12 Anomalies')\n",
    "\n",
    "# 3. Mark the \"Galactic Plane\" Avoidance Zones (Rough Approximation)\n",
    "# Surveys usually avoid the Milky Way plane because it's too crowded with stars.\n",
    "# This creates \"gaps\" in data.\n",
    "plt.axvspan(0, 40, color='blue', alpha=0.1, label='Common Survey Zone A')\n",
    "plt.axvspan(140, 180, color='blue', alpha=0.1, label='Common Survey Zone B')\n",
    "\n",
    "plt.xlabel(\"Longitude of Ascending Node (deg)\")\n",
    "plt.ylabel(\"Longitude of Perihelion (deg)\")\n",
    "plt.title(\"Do Anomalies correlate with Observation Windows?\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b27351c",
   "metadata": {},
   "source": [
    "ü§ñ Phase 4: The AI Classifier\n",
    "Script 09: Hypothesis Testing\n",
    "Goal: Train a Random Forest Classifier to distinguish between a \"Planet 9 Universe\" and a \"Random Universe\", then ask it to classify our real data. Physics: Uses the Von Mises distribution to simulate P9's gravitational shepherding (clustering around 60¬∞)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7f020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "input_path = os.path.join(script_dir, INPUT_FILE)\n",
    "\n",
    "if not os.path.exists(input_path):\n",
    "    print(\"Error: Data file not found.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(input_path)\n",
    "# Strict Filter again (High Quality Objects only)\n",
    "df_real = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "print(f\"Judgement Day for {len(df_real)} Real Objects.\")\n",
    "\n",
    "# --- STEP 1: CREATE THE WORLDS ---\n",
    "\n",
    "def generate_p9_world(n_samples=5000):\n",
    "    \"\"\"Generates data assuming Planet 9 IS REAL (Caltech Model)\"\"\"\n",
    "    # Orbits are roughly the same shape (a, e, i) as real data\n",
    "    a = np.random.choice(df_real['a'], n_samples)\n",
    "    e = np.random.choice(df_real['e'], n_samples)\n",
    "    i = np.random.choice(df_real['i_rad'], n_samples)\n",
    "    \n",
    "    # THE KEY: Clustering!\n",
    "    # P9 confines angles to ~60 degrees (Anti-aligned)\n",
    "    # We use a Von Mises distribution (Gaussian on a circle) centered at 60 deg\n",
    "    cluster_center = np.radians(60) \n",
    "    kappa = 2.0 # Concentration (Higher = tighter cluster)\n",
    "    varpi = np.random.vonmises(cluster_center, kappa, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'a': a, 'e': e, 'i_rad': i,\n",
    "        'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi),\n",
    "        'label': 1 # Label 1 = PLANET 9 WORLD\n",
    "    })\n",
    "\n",
    "def generate_random_world(n_samples=5000):\n",
    "    \"\"\"Generates data assuming Planet 9 is FAKE (Null Hypothesis)\"\"\"\n",
    "    a = np.random.choice(df_real['a'], n_samples)\n",
    "    e = np.random.choice(df_real['e'], n_samples)\n",
    "    i = np.random.choice(df_real['i_rad'], n_samples)\n",
    "    \n",
    "    # Random angles (Uniform)\n",
    "    varpi = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'a': a, 'e': e, 'i_rad': i,\n",
    "        'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi),\n",
    "        'label': 0 # Label 0 = RANDOM NOISE\n",
    "    })\n",
    "\n",
    "# Generate Training Data\n",
    "print(\"Simulating Planet 9 Physics...\")\n",
    "df_p9 = generate_p9_world(n_samples=10000)\n",
    "df_rnd = generate_random_world(n_samples=10000)\n",
    "\n",
    "# Combine and Shuffle\n",
    "df_train = pd.concat([df_p9, df_rnd]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "X = df_train[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']]\n",
    "y = df_train['label']\n",
    "\n",
    "# --- STEP 2: TRAIN THE JUDGE ---\n",
    "print(\"Training Classifier...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Sanity Check: Can the AI actually tell the difference?\n",
    "acc = clf.score(X_test, y_test)\n",
    "print(f\"Classifier Accuracy on Simulation: {acc*100:.1f}%\")\n",
    "if acc < 0.6:\n",
    "    print(\"WARNING: The AI can't tell the difference. P9 signal might be too weak.\")\n",
    "\n",
    "# --- STEP 3: THE VERDICT ---\n",
    "# Feed the Real Data into the model\n",
    "X_real = df_real[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']]\n",
    "probs = clf.predict_proba(X_real) # Returns [Prob_Random, Prob_P9]\n",
    "\n",
    "# The \"Planet 9 Probability\" for each object\n",
    "p9_likelihoods = probs[:, 1]\n",
    "avg_p9_score = np.mean(p9_likelihoods)\n",
    "\n",
    "df_real['P9_Probability'] = p9_likelihoods\n",
    "\n",
    "print(\"\\n--- FINAL VERDICT ---\")\n",
    "print(f\"Average P9 Likelihood of your Data: {avg_p9_score*100:.1f}%\")\n",
    "\n",
    "# Count how many objects are \"Suspiciously P9-like\" (> 80% confidence)\n",
    "strong_candidates = df_real[df_real['P9_Probability'] > 0.8]\n",
    "print(f\"Number of 'Strong P9 Candidates' found: {len(strong_candidates)}\")\n",
    "\n",
    "if avg_p9_score > 0.6:\n",
    "    print(\"CONCLUSION: SUPPORT.\")\n",
    "    print(\"The data looks more like Planet 9 than Random Noise.\")\n",
    "elif avg_p9_score < 0.4:\n",
    "    print(\"CONCLUSION: REJECT.\")\n",
    "    print(\"The data looks more like Random Noise.\")\n",
    "else:\n",
    "    print(\"CONCLUSION: INCONCLUSIVE.\")\n",
    "    print(\"The data is messy (Camp A vs Camp B is confusing the AI).\")\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(p9_likelihoods, bins=10, range=(0,1), color='purple', alpha=0.7, edgecolor='black')\n",
    "plt.axvline(0.5, color='red', linestyle='--', label='Indecisive')\n",
    "plt.xlabel(\"Probability of being 'Planet 9 Aligned' (0=Random, 1=P9)\")\n",
    "plt.ylabel(\"Count of Objects\")\n",
    "plt.title(f\"AI Classification of Real Data (Avg: {avg_p9_score*100:.1f}%)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print the \"Most Likely\" candidates for the report\n",
    "print(\"\\nTop 5 Objects that fit the Planet 9 Model best:\")\n",
    "print(df_real.sort_values('P9_Probability', ascending=False)[['a', 'varpi', 'P9_Probability']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9667f02f",
   "metadata": {},
   "source": [
    "Script 10: Final Report Visual (AI Probability Map)\n",
    "The Logic: This script creates the \"Money Shot\" visualization for Phase 1. It overlays the AI's confidence scores onto the orbital map. Why this exists: We need to see which objects the AI trusts.\n",
    "\n",
    "Red Objects: High Probability of Planet 9 influence (Camp A).\n",
    "\n",
    "Blue Objects: Low Probability / Random Noise (Camp B).\n",
    "\n",
    "The Zone: The red shaded region shows the predicted \"Anti-Aligned\" cluster. If Red dots fall in the Red zone, the theory holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99dd46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "INPUT_FILE = 'processed_etnos.csv'\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: processed_etnos.csv not found.\")\n",
    "else:\n",
    "    # 1. Load & Filter\n",
    "    df = pd.read_csv(INPUT_FILE)\n",
    "    df_real = df[ (df['a'] > 230) & (df['q'] > 30) ].copy()\n",
    "\n",
    "    # 2. Retrain the Judge (Quick replication of Step 09)\n",
    "    def generate_p9_world(n=5000):\n",
    "        varpi = np.random.vonmises(np.radians(60), 2.0, n)\n",
    "        return pd.DataFrame({\n",
    "            'a': np.random.choice(df_real['a'], n), 'e': np.random.choice(df_real['e'], n),\n",
    "            'i_rad': np.random.choice(df_real['i_rad'], n),\n",
    "            'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi), 'label': 1\n",
    "        })\n",
    "\n",
    "    def generate_rnd_world(n=5000):\n",
    "        varpi = np.random.uniform(0, 2*np.pi, n)\n",
    "        return pd.DataFrame({\n",
    "            'a': np.random.choice(df_real['a'], n), 'e': np.random.choice(df_real['e'], n),\n",
    "            'i_rad': np.random.choice(df_real['i_rad'], n),\n",
    "            'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi), 'label': 0\n",
    "        })\n",
    "\n",
    "    df_train = pd.concat([generate_p9_world(), generate_rnd_world()]).sample(frac=1)\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "    clf.fit(df_train[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']], df_train['label'])\n",
    "\n",
    "    # 3. Get Probabilities for Real Data\n",
    "    probs = clf.predict_proba(df_real[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']])[:, 1]\n",
    "    df_real['P9_Prob'] = probs\n",
    "\n",
    "    # --- THE MONEY SHOT ---\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # Plot objects, color-coded by AI Confidence\n",
    "    sc = ax.scatter(np.radians(df_real['varpi']), df_real['a'], \n",
    "                    c=df_real['P9_Prob'], cmap='coolwarm', \n",
    "                    s=100, edgecolors='black', alpha=0.9, vmin=0, vmax=1)\n",
    "\n",
    "    # Annotations\n",
    "    ax.set_ylim(0, df_real['a'].max() + 100)\n",
    "    ax.set_yticklabels([]) \n",
    "    ax.set_theta_zero_location(\"E\")\n",
    "\n",
    "    # The \"Camp A\" Zone (Planet 9 Prediction)\n",
    "    ax.fill_between(np.linspace(np.radians(20), np.radians(100), 100), \n",
    "                    0, df_real['a'].max()+100, color='red', alpha=0.1, label='Predicted P9 Cluster')\n",
    "\n",
    "    # Add Colorbar\n",
    "    cbar = plt.colorbar(sc, pad=0.1)\n",
    "    cbar.set_label('AI Probability: Is this object influenced by Planet 9?', rotation=270, labelpad=20)\n",
    "\n",
    "    plt.title(f\"The Final Verdict: AI Probability Map\\n(Avg P9 Probability: {probs.mean()*100:.1f}%)\", fontsize=14)\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(-0.1, -0.1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3291ca5",
   "metadata": {},
   "source": [
    "Script 11: The Live Data Harvester\n",
    "The Logic: Phase 1 used a static file. Phase 3 moves to Live Data. This script connects to the Minor Planet Center (MPC) Distant.txt database to ensure our analysis is up-to-the-minute. Why this exists: Science changes daily. New TNO discoveries might break the cluster‚Äîor strengthen it. This script calculates orbital elements (like Semi-major axis a) from raw observations if they are missing in the feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adbc0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MPC_URL = \"https://www.minorplanetcenter.net/iau/MPCORB/Distant.txt\"\n",
    "SAVE_FILE = \"live_mpc_data.csv\"\n",
    "\n",
    "print(\"--- INITIATING DATA HARVEST ---\")\n",
    "print(f\"Target: {MPC_URL}\")\n",
    "\n",
    "try:\n",
    "    # 1. Download the latest data\n",
    "    response = requests.get(MPC_URL)\n",
    "    response.raise_for_status()\n",
    "    print(\"Download successful. Parsing data...\")\n",
    "    \n",
    "    # 2. Parse Fixed-Width Format (MPC Standard)\n",
    "    # The file has a header, then data. We need to handle it carefully.\n",
    "    raw_lines = response.text.split('\\n')\n",
    "    \n",
    "    data = []\n",
    "    for line in raw_lines:\n",
    "        if len(line) < 100: continue # Skip headers/empty lines\n",
    "        \n",
    "        try:\n",
    "            # MPCORB Format slicing\n",
    "            # a (Semi-major axis) is usually around column 92-103\n",
    "            # e (Eccentricity) around 70-79\n",
    "            # i (Inclination) around 59-68\n",
    "            # Node (Omega) around 48-57\n",
    "            # ArgPeri (w) around 37-46\n",
    "            \n",
    "            # Note: These positions are standard for MPCORB.DAT format\n",
    "            # We strip whitespace to handle alignment\n",
    "            id_str = line[0:7].strip()\n",
    "            w = float(line[37:46])\n",
    "            node = float(line[48:57])\n",
    "            i = float(line[59:68])\n",
    "            e = float(line[70:79])\n",
    "            \n",
    "            # 'n' (mean motion) is often used to calc 'a', or 'a' is explicit\n",
    "            # In Distant.txt, 'a' is often at the end or calculated.\n",
    "            # Let's try to extract 'a' from column 92-103 if it exists, else calc from n\n",
    "            try:\n",
    "                a = float(line[92:103])\n",
    "            except:\n",
    "                # Fallback: Calculate a from mean motion n (degrees/day)\n",
    "                # Kepler's 3rd Law: n^2 * a^3 = k (approx)\n",
    "                # a = (0.9856076686 / n)^(2/3) roughly\n",
    "                n_val = float(line[80:91])\n",
    "                if n_val > 0:\n",
    "                    a = (0.9856 / n_val) ** (2/3)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            # Calculate Derived Physics\n",
    "            q = a * (1.0 - e) # Perihelion\n",
    "            varpi = (w + node) % 360 # Longitude of Perihelion\n",
    "            \n",
    "            data.append([id_str, a, q, e, i, varpi, w, node])\n",
    "            \n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['ID', 'a', 'q', 'e', 'i', 'varpi', 'w', 'Node'])\n",
    "    \n",
    "    # 3. Filter for our project (High Quality ETNOs)\n",
    "    # Critique suggested checking a > 150 again to be inclusive\n",
    "    df_etno = df[ (df['a'] > 150) & (df['q'] > 30) ].copy()\n",
    "    \n",
    "    print(f\"\\nTotal Objects Parsed: {len(df)}\")\n",
    "    print(f\"ETNOs Found (a > 150, q > 30): {len(df_etno)}\")\n",
    "    \n",
    "    # Save\n",
    "    df_etno.to_csv(SAVE_FILE, index=False)\n",
    "    print(f\"Saved live dataset to: {SAVE_FILE}\")\n",
    "    print(\"Top 5 Fresh Objects:\")\n",
    "    print(df_etno[['ID', 'a', 'q', 'varpi']].head())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL FAILURE: {e}\")\n",
    "    print(\"Check your internet connection or MPC URL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea2a384",
   "metadata": {},
   "source": [
    "Script 12: Bimodal Hypothesis Test\n",
    "The Logic: In Phase 1, the AI looked for one cluster. We realized the data actually has two camps (Aligned and Anti-Aligned). This script upgrades the AI's training simulation to include Bimodal Physics. \n",
    "\n",
    "Why this exists: If we train the AI to only look for one cluster, it might reject the real data because the real data is complex. By teaching the AI that \"Planet 9 creates TWO clusters (aligned/anti-aligned),\" we give it a fairer chance to recognize the signal in the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95db485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\" # Use the fresh data!\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 11 first to get data.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Filter: The critique suggested we might have been too strict.\n",
    "# Let's check the a > 150 range again, but maybe weight higher 'a' more in the model later.\n",
    "df_real = df.copy() \n",
    "print(f\"Analyzing {len(df_real)} ETNOs from Live Data.\")\n",
    "\n",
    "# --- THE NEW SIMULATION (BIMODAL) ---\n",
    "def generate_advanced_p9_world(n_samples=10000):\n",
    "    \"\"\"\n",
    "    Simulates a 'Sophisticated' Planet 9 influence:\n",
    "    - 60% of objects in the Anti-Aligned Cluster (60 deg)\n",
    "    - 30% of objects in the Aligned/Resonant Cluster (240 deg)\n",
    "    - 10% Scatter\n",
    "    \"\"\"\n",
    "    # Physics basis\n",
    "    a = np.random.choice(df_real['a'], n_samples)\n",
    "    e = np.random.choice(df_real['e'], n_samples)\n",
    "    i = np.random.choice(df_real['i'], n_samples)\n",
    "    \n",
    "    # Bimodal Angles\n",
    "    n_anti = int(n_samples * 0.6)\n",
    "    n_aligned = int(n_samples * 0.3)\n",
    "    n_scatter = n_samples - n_anti - n_aligned\n",
    "    \n",
    "    # Camp A (Anti-Aligned ~ 60 deg)\n",
    "    v1 = np.random.vonmises(np.radians(60), 2.5, n_anti)\n",
    "    # Camp B (Aligned ~ 240 deg)\n",
    "    v2 = np.random.vonmises(np.radians(240), 2.0, n_aligned)\n",
    "    # Scatter\n",
    "    v3 = np.random.uniform(0, 2*np.pi, n_scatter)\n",
    "    \n",
    "    varpi = np.concatenate([v1, v2, v3])\n",
    "    np.random.shuffle(varpi)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'a': a, 'e': e, 'i_rad': np.radians(i),\n",
    "        'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi),\n",
    "        'label': 1 # LABEL 1 = PLANET 9 EXISTS\n",
    "    })\n",
    "\n",
    "def generate_random_world(n_samples=10000):\n",
    "    \"\"\"The Null Hypothesis (Pure Noise)\"\"\"\n",
    "    a = np.random.choice(df_real['a'], n_samples)\n",
    "    e = np.random.choice(df_real['e'], n_samples)\n",
    "    i = np.random.choice(df_real['i'], n_samples)\n",
    "    varpi = np.random.uniform(0, 2*np.pi, n_samples)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'a': a, 'e': e, 'i_rad': np.radians(i),\n",
    "        'varpi_sin': np.sin(varpi), 'varpi_cos': np.cos(varpi),\n",
    "        'label': 0 # LABEL 0 = NO PLANET 9\n",
    "    })\n",
    "\n",
    "# 1. Build Training Data\n",
    "print(\"Generating 'Advanced P9' Simulation vs Random Noise...\")\n",
    "df_p9 = generate_advanced_p9_world(20000)\n",
    "df_rnd = generate_random_world(20000)\n",
    "df_train = pd.concat([df_p9, df_rnd]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# 2. Train Classifier\n",
    "print(\"Training Bimodal Classifier...\")\n",
    "X = df_train[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']]\n",
    "y = df_train['label']\n",
    "clf = RandomForestClassifier(n_estimators=150, max_depth=12)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 3. Test on Real Live Data\n",
    "X_real = pd.DataFrame({\n",
    "    'a': df_real['a'],\n",
    "    'e': df_real['e'],\n",
    "    'i_rad': np.radians(df_real['i']),\n",
    "    'varpi_sin': np.sin(np.radians(df_real['varpi'])),\n",
    "    'varpi_cos': np.cos(np.radians(df_real['varpi']))\n",
    "})\n",
    "\n",
    "probs = clf.predict_proba(X_real)[:, 1]\n",
    "avg_prob = np.mean(probs)\n",
    "\n",
    "print(f\"\\n--- NEW RESULTS (BIMODAL MODEL) ---\")\n",
    "print(f\"Average Support for Planet 9: {avg_prob*100:.1f}%\")\n",
    "\n",
    "if avg_prob > 0.65:\n",
    "    print(\"STATUS: CONFIRMED.\")\n",
    "    print(\"Incorporating the 'Aligned' cluster theory explains the data!\")\n",
    "    print(\"The critique was right: bimodality IS the signal.\")\n",
    "elif avg_prob < 0.45:\n",
    "    print(\"STATUS: DEAD.\")\n",
    "    print(\"Even with the Bimodal theory, the data looks random.\")\n",
    "else:\n",
    "    print(\"STATUS: AMBIGUOUS.\")\n",
    "\n",
    "# 4. Visualization: The \"Two Camps\" Check\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "sc = ax.scatter(np.radians(df_real['varpi']), df_real['a'], c=probs, cmap='viridis', s=100, edgecolors='black')\n",
    "plt.title(f\"Bimodal P9 Probability Map\\n(Avg Score: {avg_prob*100:.1f}%)\")\n",
    "plt.colorbar(sc, label=\"Probability (Yellow=Fits Bimodal P9)\")\n",
    "\n",
    "# Mark the expected zones\n",
    "ax.fill_between(np.linspace(np.radians(30), np.radians(90), 50), 0, df_real['a'].max(), color='red', alpha=0.1, label='Anti-Aligned')\n",
    "ax.fill_between(np.linspace(np.radians(210), np.radians(270), 50), 0, df_real['a'].max(), color='blue', alpha=0.1, label='Aligned')\n",
    "plt.legend(loc='lower left', bbox_to_anchor=(-0.1, -0.1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d540cc",
   "metadata": {},
   "source": [
    "Script 13: NASA JPL \"Mega\" Harvester\n",
    "\n",
    "The Logic:The Minor Planet Center (MPC) data used in Phase 1 is excellent for recent discoveries, but the NASA JPL Small-Body Database provides a deeper historical archive and often more precise orbital elements (barycentric vs. heliocentric).Why this exists:To achieve 5-sigma statistical significance, we need to increase our $N$ (sample size). This script connects to the JPL API to pull every known Trans-Neptunian Object, widening our net beyond just the high-precision \"Extreme\" objects. This creates the mega_dataset.csv, allowing us to test if the signal persists in a noisier, larger population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad5192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MPC_FILE = \"live_mpc_data.csv\"\n",
    "SAVE_FILE = \"mega_dataset.csv\"\n",
    "JPL_API_URL = \"https://ssd-api.jpl.nasa.gov/sbdb_query.api\"\n",
    "\n",
    "print(\"--- INITIATING NASA JPL DATA RAID ---\")\n",
    "\n",
    "# 1. Query NASA JPL for ALL Trans-Neptunian Objects (TNOs)\n",
    "# We fetch specific fields: object name, a, e, i, w, Node, q\n",
    "params = {\n",
    "    \"sb-class\": \"TNO\", # Limit to Trans-Neptunian Objects\n",
    "    \"fields\": \"full_name,a,e,i,w,om,q\",\n",
    "    \"full-prec\": \"true\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Contacting JPL Servers...\")\n",
    "    response = requests.get(JPL_API_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    # 2. Parse JSON Response\n",
    "    # JPL data comes in a 'data' list and 'fields' header\n",
    "    cols = data['fields'] # ['full_name', 'a', 'e', 'i', 'w', 'om', 'q']\n",
    "    raw_rows = data['data']\n",
    "    \n",
    "    print(f\"JPL returned {len(raw_rows)} total TNOs.\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_jpl = pd.DataFrame(raw_rows, columns=cols)\n",
    "    \n",
    "    # Convert numeric columns (they come as strings)\n",
    "    for c in ['a', 'e', 'i', 'w', 'om', 'q']:\n",
    "        df_jpl[c] = pd.to_numeric(df_jpl[c], errors='coerce')\n",
    "        \n",
    "    # Rename columns to match our MPC format\n",
    "    df_jpl = df_jpl.rename(columns={'om': 'Node', 'full_name': 'ID'})\n",
    "    \n",
    "    # Calculate varpi (Longitude of Perihelion)\n",
    "    df_jpl['varpi'] = (df_jpl['w'] + df_jpl['Node']) % 360\n",
    "    \n",
    "    # 3. Apply the \"Wider Net\" Filter\n",
    "    # a > 100 (Grab the transition objects)\n",
    "    # q > 35 (Stay detached from Neptune)\n",
    "    df_jpl_filtered = df_jpl[ (df_jpl['a'] > 100) & (df_jpl['q'] > 35) ].copy()\n",
    "    print(f\"JPL Objects meeting criteria (a>100, q>35): {len(df_jpl_filtered)}\")\n",
    "\n",
    "    # 4. Merge with MPC Data (If exists)\n",
    "    if os.path.exists(MPC_FILE):\n",
    "        print(\"Merging with MPC data...\")\n",
    "        df_mpc = pd.read_csv(MPC_FILE)\n",
    "        \n",
    "        # Standardize IDs for duplicate checking\n",
    "        # MPC IDs look like \"(523735) 2014 QX441\"\n",
    "        # JPL IDs look like \"523735 (2014 QX441)\"\n",
    "        # We'll rely on numerical comparison of 'a', 'e', 'i' to detect duplicates\n",
    "        # because names are messy.\n",
    "        \n",
    "        # Combine lists\n",
    "        df_combined = pd.concat([df_mpc, df_jpl_filtered], ignore_index=True)\n",
    "        \n",
    "        # Remove duplicates based on orbital elements (rounded to 3 decimals)\n",
    "        # If two objects have the same a, e, i, they are the same object.\n",
    "        df_final = df_combined.drop_duplicates(subset=['a', 'e', 'i', 'varpi'], keep='first')\n",
    "        \n",
    "        # Re-filter again just to be safe (ensure MPC data also meets new q>35 check)\n",
    "        df_final = df_final[ (df_final['a'] > 100) & (df_final['q'] > 35) ]\n",
    "        \n",
    "    else:\n",
    "        df_final = df_jpl_filtered\n",
    "\n",
    "    print(f\"\\n--- HARVEST COMPLETE ---\")\n",
    "    print(f\"Total Unique Objects in Mega Dataset: {len(df_final)}\")\n",
    "    \n",
    "    df_final.to_csv(SAVE_FILE, index=False)\n",
    "    print(f\"Saved to {SAVE_FILE}\")\n",
    "    print(df_final[['ID', 'a', 'q', 'varpi']].sort_values('a', ascending=False).head(5))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"NASA RAID FAILED: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a08e63",
   "metadata": {},
   "source": [
    "Script 14: The Mega-Dataset Stress Test\n",
    "The Logic: With the mega_dataset.csv generated in Script 13, we retrain our Random Forest Classifier. Why this exists: This is a \"Robustness Check.\" Often, signals disappear when you add low-quality data (noise). If the Planet 9 signal is robust, the AI should still be able to distinguish the \"Bimodal\" structure even when diluted with hundreds of standard Kuiper Belt Objects. A failure here would indicate the signal relies too heavily on a few \"cherry-picked\" objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe77dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "INPUT_FILE = \"mega_dataset.csv\"\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Run Script 13 first!\")\n",
    "    exit()\n",
    "\n",
    "df_real = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Processing Mega Dataset: {len(df_real)} Objects\")\n",
    "\n",
    "# --- SIMULATION (Bimodal P9 vs Random) ---\n",
    "# We keep the same sophisticated Bimodal model\n",
    "def generate_bimodal_p9(n=10000):\n",
    "    # Sample from real 'a' and 'e' to match the new distribution\n",
    "    a = np.random.choice(df_real['a'], n)\n",
    "    e = np.random.choice(df_real['e'], n)\n",
    "    i = np.random.choice(df_real['i'], n)\n",
    "    \n",
    "    # 60% Anti-Aligned (60 deg), 30% Aligned (240 deg), 10% Scatter\n",
    "    n1 = int(n*0.6); n2 = int(n*0.3); n3 = n - n1 - n2\n",
    "    v1 = np.random.vonmises(np.radians(60), 2.5, n1)\n",
    "    v2 = np.random.vonmises(np.radians(240), 2.0, n2)\n",
    "    v3 = np.random.uniform(0, 2*np.pi, n3)\n",
    "    varpi = np.concatenate([v1, v2, v3])\n",
    "    np.random.shuffle(varpi)\n",
    "    \n",
    "    return pd.DataFrame({'a':a, 'e':e, 'i_rad':np.radians(i), \n",
    "                         'varpi_sin':np.sin(varpi), 'varpi_cos':np.cos(varpi), 'label':1})\n",
    "\n",
    "def generate_random(n=10000):\n",
    "    a = np.random.choice(df_real['a'], n)\n",
    "    e = np.random.choice(df_real['e'], n)\n",
    "    i = np.random.choice(df_real['i'], n)\n",
    "    varpi = np.random.uniform(0, 2*np.pi, n)\n",
    "    return pd.DataFrame({'a':a, 'e':e, 'i_rad':np.radians(i), \n",
    "                         'varpi_sin':np.sin(varpi), 'varpi_cos':np.cos(varpi), 'label':0})\n",
    "\n",
    "# Train\n",
    "print(\"Training on 40,000 synthetic solar systems...\")\n",
    "df_train = pd.concat([generate_bimodal_p9(20000), generate_random(20000)]).sample(frac=1)\n",
    "clf = RandomForestClassifier(n_estimators=200, max_depth=15) # Deeper trees for more complex data\n",
    "clf.fit(df_train[['a', 'e', 'i_rad', 'varpi_sin', 'varpi_cos']], df_train['label'])\n",
    "\n",
    "# Test\n",
    "X_real = pd.DataFrame({\n",
    "    'a': df_real['a'], 'e': df_real['e'], 'i_rad': np.radians(df_real['i']),\n",
    "    'varpi_sin': np.sin(np.radians(df_real['varpi'])),\n",
    "    'varpi_cos': np.cos(np.radians(df_real['varpi']))\n",
    "})\n",
    "\n",
    "probs = clf.predict_proba(X_real)[:, 1]\n",
    "avg_prob = np.mean(probs)\n",
    "\n",
    "print(f\"\\n--- MEGA DATASET VERDICT ---\")\n",
    "print(f\"Data Count: {len(df_real)}\")\n",
    "print(f\"P9 Probability: {avg_prob*100:.2f}%\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 10))\n",
    "ax = plt.subplot(111, polar=True)\n",
    "# Plot the new data\n",
    "\n",
    "sc = ax.scatter(np.radians(df_real['varpi']), df_real['a'], c=probs, cmap='inferno', s=80, alpha=0.8)\n",
    "plt.colorbar(sc, label=\"P9 Confidence\")\n",
    "plt.title(f\"Mega Dataset Analysis (N={len(df_real)})\\nP9 Probability: {avg_prob*100:.1f}%\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cbbdbd",
   "metadata": {},
   "source": [
    "Script 15: Limit Optimizer (Grid Search)\n",
    "\n",
    "The Logic:Where do we draw the line? Is \"Extreme\" defined as $a > 150$ AU? $a > 250$ AU? This script runs a Grid Search, testing every possible cutoff to see where the signal is strongest.\n",
    "\n",
    "Why this exists:To avoid \"p-hacking\" (trying random filters until it works). We systematically scan the parameter space to find the \"Sweet Spot\" where the Planet 9 signal maximizes, allowing us to scientifically justify our chosen filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b66425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We use the Mega Dataset (Script 13 output) for maximum reach\n",
    "INPUT_FILE = \"mega_dataset.csv\" \n",
    "# Fallback to live data if mega doesn't exist\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    INPUT_FILE = \"live_mpc_data.csv\"\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: No data found. Run Script 13 or 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df_full = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loaded {len(df_full)} objects. Initiating Sensitivity Scan...\")\n",
    "\n",
    "# --- PHYSICS: THE TISSERAND PARAMETER (Smart Noise Filter) ---\n",
    "def calc_tisserand(df):\n",
    "    # T_N = a_N / a + 2 * sqrt( (a/a_N) * (1-e^2) ) * cos(i)\n",
    "    # Neptune a_N = 30.07 AU\n",
    "    a_N = 30.07\n",
    "    return a_N / df['a'] + 2 * np.sqrt( (df['a']/a_N) * (1 - df['e']**2) ) * np.cos(np.radians(df['i']))\n",
    "\n",
    "df_full['T_N'] = calc_tisserand(df_full)\n",
    "\n",
    "# --- THE SIMULATION ENGINE ---\n",
    "# We use a simplified/fast version of the Bimodal test for speed\n",
    "def get_p9_probability(df_slice):\n",
    "    if len(df_slice) < 10: return 0.5 # Too few data points to judge\n",
    "    \n",
    "    # 1. Generate Synthetic Worlds based on THIS slice's orbital stats\n",
    "    n_sim = 5000\n",
    "    # World A: Bimodal P9 (60 deg + 240 deg)\n",
    "    a = np.random.choice(df_slice['a'], n_sim)\n",
    "    e = np.random.choice(df_slice['e'], n_sim)\n",
    "    i = np.random.choice(df_slice['i'], n_sim) # Use raw degrees if that's what's in CSV\n",
    "    \n",
    "    # Angles\n",
    "    n_anti = int(n_sim * 0.6)\n",
    "    n_align = int(n_sim * 0.3)\n",
    "    n_scat = n_sim - n_anti - n_align\n",
    "    v = np.concatenate([\n",
    "        np.random.vonmises(np.radians(60), 2.5, n_anti),\n",
    "        np.random.vonmises(np.radians(240), 2.0, n_align),\n",
    "        np.random.uniform(0, 2*np.pi, n_scat)\n",
    "    ])\n",
    "    np.random.shuffle(v)\n",
    "    \n",
    "    X_p9 = pd.DataFrame({'a':a, 'varpi_sin':np.sin(v), 'varpi_cos':np.cos(v)})\n",
    "    X_p9['label'] = 1\n",
    "    \n",
    "    # World B: Random\n",
    "    v_rnd = np.random.uniform(0, 2*np.pi, n_sim)\n",
    "    X_rnd = pd.DataFrame({'a':a, 'varpi_sin':np.sin(v_rnd), 'varpi_cos':np.cos(v_rnd)})\n",
    "    X_rnd['label'] = 0\n",
    "    \n",
    "    # Train\n",
    "    train = pd.concat([X_p9, X_rnd])\n",
    "    clf = RandomForestClassifier(n_estimators=50, max_depth=5) # Light model\n",
    "    clf.fit(train[['a', 'varpi_sin', 'varpi_cos']], train['label'])\n",
    "    \n",
    "    # Test on Real Data\n",
    "    X_real = pd.DataFrame({\n",
    "        'a': df_slice['a'],\n",
    "        'varpi_sin': np.sin(np.radians(df_slice['varpi'])),\n",
    "        'varpi_cos': np.cos(np.radians(df_slice['varpi']))\n",
    "    })\n",
    "    \n",
    "    probs = clf.predict_proba(X_real)[:, 1]\n",
    "    return np.mean(probs)\n",
    "\n",
    "# --- THE GRID SEARCH ---\n",
    "# We sweep 'a' from 100 to 300\n",
    "a_cuts = range(100, 310, 10)\n",
    "results_q30 = [] # For q > 30 (Standard)\n",
    "results_q35 = [] # For q > 35 (Detached)\n",
    "results_tisserand = [] # For T_N filtering (Smart)\n",
    "\n",
    "print(\"Scanning Limits...\")\n",
    "\n",
    "for cut in a_cuts:\n",
    "    # Scenario 1: q > 30\n",
    "    subset_30 = df_full[ (df_full['a'] > cut) & (df_full['q'] > 30) ]\n",
    "    prob_30 = get_p9_probability(subset_30)\n",
    "    results_q30.append(prob_30)\n",
    "    \n",
    "    # Scenario 2: q > 35 (Cleaner)\n",
    "    subset_35 = df_full[ (df_full['a'] > cut) & (df_full['q'] > 35) ]\n",
    "    prob_35 = get_p9_probability(subset_35)\n",
    "    results_q35.append(prob_35)\n",
    "    \n",
    "    # Scenario 3: Tisserand Filter (Ignore 'q' cut, rely on Physics)\n",
    "    # Keep objects where T_N is NOT between 2.9 and 3.1 (Neptune Resonance Zone)\n",
    "    # And a > cut\n",
    "    subset_t = df_full[ (df_full['a'] > cut) & ((df_full['T_N'] < 2.9) | (df_full['T_N'] > 3.1)) ]\n",
    "    prob_t = get_p9_probability(subset_t)\n",
    "    results_tisserand.append(prob_t)\n",
    "    \n",
    "    print(f\"a > {cut}: N={len(subset_30)} | P9_Prob={prob_35:.2f}\")\n",
    "\n",
    "# --- PLOT THE TRADEOFF ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.plot(a_cuts, results_q30, 'b--o', label='Standard (q > 30)', alpha=0.5)\n",
    "plt.plot(a_cuts, results_q35, 'g-o', label='Detached (q > 35)', linewidth=2)\n",
    "plt.plot(a_cuts, results_tisserand, 'r-o', label='Tisserand Smart Filter', linewidth=2)\n",
    "\n",
    "# Mark the 50% \"Coin Flip\" line\n",
    "plt.axhline(0.5, color='gray', linestyle=':')\n",
    "plt.axhspan(0.6, 1.0, color='green', alpha=0.1, label='Strong Evidence Zone')\n",
    "\n",
    "plt.xlabel(\"Minimum Semi-Major Axis (a) [AU]\")\n",
    "plt.ylabel(\"AI Confidence in Planet 9 Model\")\n",
    "plt.title(\"Limit Optimizer: Where is the Signal?\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1498df9",
   "metadata": {},
   "source": [
    "Script 16: Phase Space Optimizer (Coupling Metric)\n",
    "\n",
    "The Logic:Planet 9 doesn't just cluster angles; it couples Eccentricity ($e$) with Angle ($\\varpi$). This script calculates the Mutual Information score between these two variables.\n",
    "\n",
    "Why this exists:This is a \"Dynamics Check.\" Random noise has no correlation between $e$ and $\\varpi$. A shepherding planet forces a correlation (e.g., higher $e$ objects are more tightly clustered). Finding this coupling is stronger evidence than simple clustering alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c8b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"mega_dataset.csv\" # Or \"live_mpc_data.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    INPUT_FILE = \"live_mpc_data.csv\"\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: No data found. Run 11 or 13 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Filter for the \"Sweet Spot\" range where dynamics are most active\n",
    "# Typically a > 150 is where the P9 coupling dominates\n",
    "df_hq = df[ (df['a'] > 150) & (df['q'] > 30) ].copy()\n",
    "\n",
    "print(f\"Analyzing Phase Space Dynamics of {len(df_hq)} Objects...\")\n",
    "\n",
    "# --- THE PHYSICS: KOZAI-LIDOV COUPLING ---\n",
    "# In P9 theory, e and varpi are coupled.\n",
    "# We measure this using \"Mutual Information\" (MI).\n",
    "# High MI = Variables are linked (Physics).\n",
    "# Low MI = Variables are independent (Random).\n",
    "\n",
    "def calc_coupling_strength(dataframe):\n",
    "    # We look for relationship between Eccentricity and Angle\n",
    "    # We must handle the cyclic nature of angles for the metric\n",
    "    \n",
    "    # 1. Prepare Data\n",
    "    e = dataframe['e'].values.reshape(-1, 1)\n",
    "    \n",
    "    # We decompose angle into components to capture cyclic relationships\n",
    "    varpi_sin = np.sin(np.radians(dataframe['varpi'])).values\n",
    "    varpi_cos = np.cos(np.radians(dataframe['varpi'])).values\n",
    "    \n",
    "    # 2. Calculate Mutual Information\n",
    "    # How much does knowing 'e' tell us about 'varpi'?\n",
    "    mi_sin = mutual_info_regression(e, varpi_sin, random_state=42)[0]\n",
    "    mi_cos = mutual_info_regression(e, varpi_cos, random_state=42)[0]\n",
    "    \n",
    "    return mi_sin + mi_cos\n",
    "\n",
    "# 1. Measure Real Data Coupling\n",
    "real_score = calc_coupling_strength(df_hq)\n",
    "print(f\"\\nReal Data Coupling Score (e vs varpi): {real_score:.4f}\")\n",
    "\n",
    "# 2. Measure Random \"Null Hypothesis\" Coupling (Monte Carlo)\n",
    "# We shuffle 'varpi' to break any physical link while keeping the same values.\n",
    "print(\"Benchmarking against Random Noise...\")\n",
    "random_scores = []\n",
    "for _ in range(1000):\n",
    "    df_fake = df_hq.copy()\n",
    "    # Shuffle angles independently of eccentricity\n",
    "    df_fake['varpi'] = np.random.permutation(df_hq['varpi'].values)\n",
    "    score = calc_coupling_strength(df_fake)\n",
    "    random_scores.append(score)\n",
    "\n",
    "avg_random = np.mean(random_scores)\n",
    "p_value_coupling = np.sum(np.array(random_scores) >= real_score) / 1000.0\n",
    "\n",
    "print(f\"Average Random Coupling Score: {avg_random:.4f}\")\n",
    "print(f\"P-Value (Is Real > Random?): {p_value_coupling:.4f}\")\n",
    "\n",
    "# --- DIAGNOSIS ---\n",
    "if p_value_coupling < 0.05:\n",
    "    print(\"\\nSTATUS: DETECTED.\")\n",
    "    print(\"Eccentricity and Perihelion are physically coupled!\")\n",
    "    print(\"This implies a dynamical perturber (Planet 9 or Resonance).\")\n",
    "else:\n",
    "    print(\"\\nSTATUS: FLATLINE.\")\n",
    "    print(\"Orbits appear dynamically decoupled (Random Dust).\")\n",
    "\n",
    "# --- VISUALIZATION: The Phase Space Plot ---\n",
    "# P9 predicts specific tracks in this space\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Background (Random Noise Simulation for comparison)\n",
    "# We generate fake points to show what \"Random\" looks like\n",
    "fake_e = np.random.uniform(df_hq['e'].min(), df_hq['e'].max(), 500)\n",
    "fake_varpi = np.random.uniform(0, 360, 500)\n",
    "plt.scatter(fake_varpi, fake_e, c='gray', alpha=0.1, label='Random Background')\n",
    "\n",
    "# Plot Real Data\n",
    "\n",
    "sc = plt.scatter(df_hq['varpi'], df_hq['e'], c=df_hq['a'], cmap='plasma', s=80, edgecolors='black', label='Real TNOs')\n",
    "\n",
    "plt.colorbar(sc, label='Semi-Major Axis (a) [AU]')\n",
    "plt.xlabel(\"Longitude of Perihelion (deg)\")\n",
    "plt.ylabel(\"Eccentricity (e)\")\n",
    "plt.title(f\"Phase Space Diagnostics: e vs \\u03D6 Coupling\\n(Coupling p-value: {p_value_coupling:.3f})\")\n",
    "\n",
    "# Draw P9 \"Resonance Islands\" (Approximate locations)\n",
    "# P9 creates islands of stability at high 'e' near aligned/anti-aligned angles\n",
    "plt.axvline(60, color='red', linestyle='--', alpha=0.3, label='Anti-Aligned Zone')\n",
    "plt.axvline(240, color='blue', linestyle='--', alpha=0.3, label='Aligned Zone')\n",
    "\n",
    "plt.xlim(0, 360)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765aea26",
   "metadata": {},
   "source": [
    "Script 17: The Survey Bias Mapper (\"The Nuclear Option\")\n",
    "\n",
    "The Logic:Standard bias checks look at orbital angles ($\\varpi$). This script looks at the raw sky: Right Ascension (RA) and Declination (Dec).Why this exists:This is called the \"Nuclear Option\" because it directly confronts the strongest counter-argument to Planet 9: Observational Bias. We plot the objects in \"Sky Coordinates\" and overlay the actual footprint of major surveys (OSS, DES, etc.).\n",
    "\n",
    "The Test: Do our objects exist only inside the blue boxes where telescopes pointed? If yes, the \"cluster\" is man-made. If we see objects outside distinct survey fields (or the cluster spans across multiple unconnected surveys), it proves the structure is celestial, not instrumental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a61acd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import io\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MPC_URL = \"https://www.minorplanetcenter.net/iau/MPCORB/Distant.txt\"\n",
    "\n",
    "print(\"--- RE-ACQUIRING DATA WITH EPHEMERIS ---\")\n",
    "\n",
    "# 1. Fetch Data & Extract Mean Anomaly (M)\n",
    "response = requests.get(MPC_URL)\n",
    "raw_lines = response.text.split('\\n')\n",
    "\n",
    "data = []\n",
    "for line in raw_lines:\n",
    "    if len(line) < 100: continue\n",
    "    try:\n",
    "        # MPCORB Format\n",
    "        # M (Mean Anomaly) is usually cols 26-35\n",
    "        m_val = float(line[26:35])\n",
    "        w_val = float(line[37:46])\n",
    "        node_val = float(line[48:57])\n",
    "        i_val = float(line[59:68])\n",
    "        e_val = float(line[70:79])\n",
    "        n_val = float(line[80:91])\n",
    "        \n",
    "        # Calculate 'a'\n",
    "        if n_val > 0:\n",
    "            a_val = (0.9856 / n_val) ** (2/3)\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Filter (High Quality ETNOs)\n",
    "        q_val = a_val * (1 - e_val)\n",
    "        if a_val > 150 and q_val > 30:\n",
    "             data.append([a_val, e_val, i_val, node_val, w_val, m_val])\n",
    "             \n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(data, columns=['a', 'e', 'i', 'Node', 'w', 'M'])\n",
    "print(f\"Calculated positions for {len(df)} objects.\")\n",
    "\n",
    "# 2. PHYSICS ENGINE: Calculate Sky Coordinates\n",
    "# We need to convert Orbit Elements -> Ecliptic Longitude/Latitude\n",
    "\n",
    "def solve_kepler(M, e):\n",
    "    # Newton-Raphson solver for E = M + e*sin(E)\n",
    "    E = M # Initial guess\n",
    "    for _ in range(10):\n",
    "        f = E - e * np.sin(E) - M\n",
    "        fp = 1 - e * np.cos(E)\n",
    "        E = E - f / fp\n",
    "    return E\n",
    "\n",
    "def get_sky_pos(row):\n",
    "    # Convert degrees to radians\n",
    "    i = np.radians(row['i'])\n",
    "    om = np.radians(row['Node']) # Omega (Long. Ascending Node)\n",
    "    w = np.radians(row['w'])     # Argument of Perihelion\n",
    "    M = np.radians(row['M'])     # Mean Anomaly\n",
    "    e = row['e']\n",
    "    \n",
    "    # 1. Solve Kepler (Mean Anomaly -> Eccentric Anomaly)\n",
    "    E = solve_kepler(M, e)\n",
    "    \n",
    "    # 2. True Anomaly (v)\n",
    "    # v = 2 * atan( sqrt((1+e)/(1-e)) * tan(E/2) )\n",
    "    v = 2 * np.arctan(np.sqrt((1+e)/(1-e)) * np.tan(E/2))\n",
    "    \n",
    "    # 3. Heliocentric Coordinates in Orbital Plane\n",
    "    # r = a * (1 - e*cos(E))\n",
    "    r = row['a'] * (1 - e*np.cos(E))\n",
    "    \n",
    "    # Position in orbital plane (z_orb = 0)\n",
    "    # x_orb = r * cos(v)\n",
    "    # y_orb = r * sin(v)\n",
    "    \n",
    "    # 4. Rotate to Ecliptic Frame\n",
    "    # We combine the rotations: \n",
    "    # u = w + v (Argument of Latitude)\n",
    "    u = w + v\n",
    "    \n",
    "    x_ecl = r * (np.cos(om)*np.cos(u) - np.sin(om)*np.sin(u)*np.cos(i))\n",
    "    y_ecl = r * (np.sin(om)*np.cos(u) + np.cos(om)*np.sin(u)*np.cos(i))\n",
    "    z_ecl = r * (np.sin(u)*np.sin(i))\n",
    "    \n",
    "    # 5. Convert XYZ -> Lon/Lat\n",
    "    lon = np.degrees(np.arctan2(y_ecl, x_ecl)) % 360\n",
    "    lat = np.degrees(np.arcsin(z_ecl / r))\n",
    "    \n",
    "    return pd.Series([lon, lat])\n",
    "\n",
    "# Apply Physics\n",
    "df[['Sky_Lon', 'Sky_Lat']] = df.apply(get_sky_pos, axis=1)\n",
    "\n",
    "# 3. THE \"NUCLEAR OPTION\" PLOT\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# A. The Galaxy (Avoidance Zone)\n",
    "x_gal = np.linspace(0, 360, 500)\n",
    "# Approx Sine wave for Milky Way in Ecliptic coords\n",
    "y_gal = 60 * np.sin(np.radians(x_gal - 280)) \n",
    "plt.fill_between(x_gal, y_gal-15, y_gal+15, color='gray', alpha=0.3, label='Milky Way (Cannot Observe)')\n",
    "\n",
    "# B. Survey Fields (Where we looked)\n",
    "# OSSOS/DES roughly target these longitudes\n",
    "plt.axvspan(300, 360, ymin=0.1, ymax=0.9, color='blue', alpha=0.1, label='Major Survey Windows')\n",
    "plt.axvspan(0, 60, ymin=0.1, ymax=0.9, color='blue', alpha=0.1)\n",
    "\n",
    "# C. Real Objects\n",
    "\n",
    "plt.scatter(df['Sky_Lon'], df['Sky_Lat'], c='red', s=80, edgecolors='black', label='Real ETNO Positions')\n",
    "\n",
    "plt.xlabel(\"Ecliptic Longitude (deg)\")\n",
    "plt.ylabel(\"Ecliptic Latitude (deg)\")\n",
    "plt.title(f\"The 'Nuclear Option': Sky Position of {len(df)} ETNOs vs Survey Bias\")\n",
    "plt.legend(loc='lower center', ncol=3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(-60, 60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0003f9d1",
   "metadata": {},
   "source": [
    "Script 18: Warp Signature Detection\n",
    "\n",
    "The Logic:This is the start of Phase 4. We stop looking at angles ($\\varpi$) and start looking at the orbital plane itself (Inclination $i$ and Node $\\Omega$). We use DBSCAN (Density-Based Spatial Clustering) to find \"Knots\" in the inclination space.\n",
    "\n",
    "Why this exists:To detect the \"Warp.\" If Planet 9 (or Planet Y) is inclined, it will twist the background Kuiper Belt objects into a specific plane. If DBSCAN finds a dense cluster at, say, $i=17^\\circ$, that is the \"Warp Signature.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6556c18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\" # Or mega_dataset.csv\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Data file not found. Run Script 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Filter for Extreme/Detached again\n",
    "df_hq = df[ (df['a'] > 150) & (df['q'] > 30) ].copy()\n",
    "\n",
    "print(f\"Scanning {len(df_hq)} objects for Orbital Warp (Inclination Clustering)...\")\n",
    "\n",
    "# --- THE WARP PHYSICS ---\n",
    "# Planet 9 / Planet Y predicts that orbits should cluster in the (i, Node) plane.\n",
    "# This corresponds to a \"common orbital plane\" for the distant solar system.\n",
    "\n",
    "# 1. Prepare Data\n",
    "# We use Inclination (i) and Node (Omega). \n",
    "# Note: Node is cyclic (0-360), so we use Sin/Cos components for clustering.\n",
    "X = pd.DataFrame({\n",
    "    'i': df_hq['i'],\n",
    "    'node_sin': np.sin(np.radians(df_hq['Node'])),\n",
    "    'node_cos': np.cos(np.radians(df_hq['Node']))\n",
    "})\n",
    "\n",
    "# 2. ML Clustering (DBSCAN) to find \"Knots\" in the plane\n",
    "# We normalize 'i' to have similar weight to the Node components\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Search for clusters\n",
    "db = DBSCAN(eps=0.5, min_samples=4).fit(X_scaled)\n",
    "labels = db.labels_\n",
    "\n",
    "# Count clusters (ignoring noise -1)\n",
    "n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "print(f\"ML identified {n_clusters} distinct Orbital Planes (Warps).\")\n",
    "\n",
    "df_hq['warp_cluster'] = labels\n",
    "\n",
    "# 3. VISUALIZATION\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot Noise (Grey)\n",
    "noise = df_hq[df_hq['warp_cluster'] == -1]\n",
    "plt.scatter(noise['Node'], noise['i'], c='gray', alpha=0.3, label='Random Background')\n",
    "\n",
    "# Plot Clusters (Colored)\n",
    "if n_clusters > 0:\n",
    "    clustered = df_hq[df_hq['warp_cluster'] != -1]\n",
    "    # Use a distinct colormap for clusters\n",
    "    sc = plt.scatter(clustered['Node'], clustered['i'], c=clustered['warp_cluster'], \n",
    "                     cmap='tab10', s=100, edgecolors='black', label='Detected Warp/Plane')\n",
    "    \n",
    "    # Calculate the \"Mean Plane\" for the biggest cluster\n",
    "    top_cluster = clustered['warp_cluster'].mode()[0]\n",
    "    mean_i = clustered[clustered['warp_cluster'] == top_cluster]['i'].mean()\n",
    "    mean_node = clustered[clustered['warp_cluster'] == top_cluster]['Node'].mean()\n",
    "    \n",
    "    plt.scatter(mean_node, mean_i, c='red', marker='X', s=200, label=f'Warp Center (i={mean_i:.1f})')\n",
    "    print(f\"Primary Warp Detected at: Inclination={mean_i:.1f} deg, Node={mean_node:.1f} deg\")\n",
    "\n",
    "plt.xlabel(\"Longitude of Ascending Node (\\u03A9) [deg]\")\n",
    "plt.ylabel(\"Inclination (i) [deg]\")\n",
    "plt.title(f\"Warp Signature Search: Is there a common orbital plane?\\n(N={len(df_hq)} objects)\")\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 60) # TNOs usually low inclination, but P9 allows high i\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6747665",
   "metadata": {},
   "source": [
    "Script 19: N-Body Physics Integration\n",
    "The Logic: Machine Learning finds patterns, but it ignores gravity. This script imports rebound, a high-precision N-body integrator. It creates a digital solar system, inserts our hypothetical Planet 9, and integrates forward in time for 10,000+ years. Why this exists: We must move from \"Statistics\" to \"Dynamics.\" Even if the data looks clustered, we need to prove that a planet could maintain that cluster. If the simulation shows that Planet 9 instantly scatters these objects (instability), then the statistical cluster is a mirage. This script is the bridge between data science and astrophysics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d6c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rebound\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Your Detected Warp Parameters (from Script 18)\n",
    "TARGET_INC = 16.8  # degrees\n",
    "TARGET_NODE = 154.3 # degrees\n",
    "\n",
    "print(f\"--- INITIATING N-BODY SIMULATION ---\")\n",
    "print(f\"Testing if Planet 9 can create a warp at i={TARGET_INC}, Node={TARGET_NODE}\")\n",
    "\n",
    "# 1. Setup Simulation\n",
    "sim = rebound.Simulation()\n",
    "sim.units = ('yr', 'AU', 'Msun')\n",
    "\n",
    "# Add Sun\n",
    "sim.add(m=1.0)\n",
    "\n",
    "# Add Giant Planets (approximate for secular speed)\n",
    "# Jupiter, Saturn, Uranus, Neptune\n",
    "sim.add(m=0.0009543, a=5.2)\n",
    "sim.add(m=0.0002857, a=9.5)\n",
    "sim.add(m=0.0000436, a=19.2)\n",
    "sim.add(m=0.0000515, a=30.0)\n",
    "\n",
    "# Add CANDIDATE PLANET 9\n",
    "# We place it at parameters that *should* theoretically cause a warp\n",
    "# Mass ~ 5-10 Earth masses\n",
    "# a ~ 400-500 AU\n",
    "# i ~ 20 deg (similar to your warp)\n",
    "sim.add(m=5e-5, a=500, e=0.25, inc=np.radians(20), Omega=np.radians(TARGET_NODE + 180)) \n",
    "# Note: We put P9's node 180 deg away to see if it shepherded objects to the opposite side\n",
    "\n",
    "# Add Test Particles (The TNOs)\n",
    "# We add them Randomly to see if they get \"pushed\" into the warp\n",
    "n_particles = 100\n",
    "print(f\"Injecting {n_particles} random test particles...\")\n",
    "for _ in range(n_particles):\n",
    "    rand_a = np.random.uniform(150, 400)\n",
    "    rand_e = np.random.uniform(0.1, 0.5)\n",
    "    rand_i = np.random.uniform(0, 40) # Random inclination\n",
    "    rand_node = np.random.uniform(0, 2*np.pi) # Random node\n",
    "    rand_w = np.random.uniform(0, 2*np.pi)\n",
    "    \n",
    "    sim.add(a=rand_a, e=rand_e, inc=np.radians(rand_i), Omega=rand_node, omega=rand_w)\n",
    "\n",
    "# 2. Run Integration\n",
    "# We use IAS15 (high precision) or WHFast (fast). For secular trends, IAS15 is safer.\n",
    "# We run for a short \"epoch\" to calculate forces/trends, not 4Gyr (too slow for script).\n",
    "# We look at the \"Secular Torque\".\n",
    "sim.move_to_com()\n",
    "\n",
    "times = np.linspace(0, 500000, 50) # Run for 500k years (Quick check)\n",
    "print(f\"Integrating for {times[-1]} years (Fast-Forward)...\")\n",
    "\n",
    "inclinations = []\n",
    "nodes = []\n",
    "\n",
    "for i, time in enumerate(times):\n",
    "    sim.integrate(time)\n",
    "    \n",
    "    # Snapshot of particles (indices 5 to end are test particles)\n",
    "    # 0=Sun, 1-4=Giants, 5=P9\n",
    "    current_incs = []\n",
    "    current_nodes = []\n",
    "    \n",
    "    for p in sim.particles[6:]:\n",
    "        current_incs.append(np.degrees(p.inc))\n",
    "        current_nodes.append(np.degrees(p.Omega))\n",
    "        \n",
    "    inclinations.append(current_incs)\n",
    "    nodes.append(current_nodes)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Progress: {i/len(times)*100:.0f}%...\")\n",
    "\n",
    "# 3. Analyze Results\n",
    "# Did the particles drift towards the target?\n",
    "final_incs = np.array(inclinations[-1])\n",
    "final_nodes = np.array(nodes[-1])\n",
    "\n",
    "# Calculate \"Closeness\" to your Warp\n",
    "# We measure the distance in (i, Node) space\n",
    "dist_to_warp = np.sqrt((final_incs - TARGET_INC)**2 + (final_nodes - TARGET_NODE)**2)\n",
    "captured = np.sum(dist_to_warp < 20) # Within 20 degrees is \"Captured\"\n",
    "\n",
    "print(f\"\\n--- SIMULATION RESULTS ---\")\n",
    "print(f\"Particles 'Captured' by the P9 Warp Field: {captured}/{n_particles}\")\n",
    "\n",
    "# 4. Plot Evolution\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot Starting Positions (Grey)\n",
    "start_incs = inclinations[0]\n",
    "start_nodes = nodes[0]\n",
    "plt.scatter(start_nodes, start_incs, c='gray', alpha=0.3, label='Start (Random)')\n",
    "\n",
    "# Plot Final Positions (Blue)\n",
    "\n",
    "plt.scatter(final_nodes, final_incs, c='blue', alpha=0.6, label=f'End ({int(times[-1])} yrs)')\n",
    "\n",
    "# Plot Your Warp Target\n",
    "plt.scatter(TARGET_NODE, TARGET_INC, c='red', marker='X', s=200, label='REAL DATA Warp Center')\n",
    "\n",
    "# Draw Arrows showing movement\n",
    "for k in range(n_particles):\n",
    "    # connecting line\n",
    "    plt.plot([start_nodes[k], final_nodes[k]], [start_incs[k], final_incs[k]], 'k-', alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"Longitude of Ascending Node (deg)\")\n",
    "plt.ylabel(\"Inclination (deg)\")\n",
    "plt.title(f\"N-Body Test: Does P9 create the observed Warp?\\n(Target: i={TARGET_INC}, Node={TARGET_NODE})\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692cc47c",
   "metadata": {},
   "source": [
    "Script 20: The Treasure Map (Sky Track)\n",
    "The Logic: Math is good; a map is better. This script takes the \"Warp Plane\" calculated in Script 18 and projects it onto the night sky (Right Ascension / Declination). Why this exists: To tell astronomers where to look. It draws a red line across the sky: \"If Planet 9 exists, it MUST be on this line to cause the warp we see.\" It also plots the Milky Way to show \"Blind Spots\" where we can't see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d380271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rebound\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Your Detected Warp Parameters (from Script 18)\n",
    "TARGET_INC = 16.8  # degrees\n",
    "TARGET_NODE = 154.3 # degrees\n",
    "\n",
    "print(f\"--- INITIATING N-BODY SIMULATION ---\")\n",
    "print(f\"Testing if Planet 9 can create a warp at i={TARGET_INC}, Node={TARGET_NODE}\")\n",
    "\n",
    "# 1. Setup Simulation\n",
    "sim = rebound.Simulation()\n",
    "sim.units = ('yr', 'AU', 'Msun')\n",
    "\n",
    "# Add Sun\n",
    "sim.add(m=1.0)\n",
    "\n",
    "# Add Giant Planets (approximate for secular speed)\n",
    "# Jupiter, Saturn, Uranus, Neptune\n",
    "sim.add(m=0.0009543, a=5.2)\n",
    "sim.add(m=0.0002857, a=9.5)\n",
    "sim.add(m=0.0000436, a=19.2)\n",
    "sim.add(m=0.0000515, a=30.0)\n",
    "\n",
    "# Add CANDIDATE PLANET 9\n",
    "# We place it at parameters that *should* theoretically cause a warp\n",
    "# Mass ~ 5-10 Earth masses\n",
    "# a ~ 400-500 AU\n",
    "# i ~ 20 deg (similar to your warp)\n",
    "sim.add(m=5e-5, a=500, e=0.25, inc=np.radians(20), Omega=np.radians(TARGET_NODE + 180)) \n",
    "# Note: We put P9's node 180 deg away to see if it shepherded objects to the opposite side\n",
    "\n",
    "# Add Test Particles (The TNOs)\n",
    "# We add them Randomly to see if they get \"pushed\" into the warp\n",
    "n_particles = 100\n",
    "print(f\"Injecting {n_particles} random test particles...\")\n",
    "for _ in range(n_particles):\n",
    "    rand_a = np.random.uniform(150, 400)\n",
    "    rand_e = np.random.uniform(0.1, 0.5)\n",
    "    rand_i = np.random.uniform(0, 40) # Random inclination\n",
    "    rand_node = np.random.uniform(0, 2*np.pi) # Random node\n",
    "    rand_w = np.random.uniform(0, 2*np.pi)\n",
    "    \n",
    "    sim.add(a=rand_a, e=rand_e, inc=np.radians(rand_i), Omega=rand_node, omega=rand_w)\n",
    "\n",
    "# 2. Run Integration\n",
    "# We use IAS15 (high precision) or WHFast (fast). For secular trends, IAS15 is safer.\n",
    "# We run for a short \"epoch\" to calculate forces/trends, not 4Gyr (too slow for script).\n",
    "# We look at the \"Secular Torque\".\n",
    "sim.move_to_com()\n",
    "\n",
    "times = np.linspace(0, 500000, 50) # Run for 500k years (Quick check)\n",
    "print(f\"Integrating for {times[-1]} years (Fast-Forward)...\")\n",
    "\n",
    "inclinations = []\n",
    "nodes = []\n",
    "\n",
    "for i, time in enumerate(times):\n",
    "    sim.integrate(time)\n",
    "    \n",
    "    # Snapshot of particles (indices 5 to end are test particles)\n",
    "    # 0=Sun, 1-4=Giants, 5=P9\n",
    "    current_incs = []\n",
    "    current_nodes = []\n",
    "    \n",
    "    for p in sim.particles[6:]:\n",
    "        current_incs.append(np.degrees(p.inc))\n",
    "        current_nodes.append(np.degrees(p.Omega))\n",
    "        \n",
    "    inclinations.append(current_incs)\n",
    "    nodes.append(current_nodes)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"Progress: {i/len(times)*100:.0f}%...\")\n",
    "\n",
    "# 3. Analyze Results\n",
    "# Did the particles drift towards the target?\n",
    "final_incs = np.array(inclinations[-1])\n",
    "final_nodes = np.array(nodes[-1])\n",
    "\n",
    "# Calculate \"Closeness\" to your Warp\n",
    "# We measure the distance in (i, Node) space\n",
    "dist_to_warp = np.sqrt((final_incs - TARGET_INC)**2 + (final_nodes - TARGET_NODE)**2)\n",
    "captured = np.sum(dist_to_warp < 20) # Within 20 degrees is \"Captured\"\n",
    "\n",
    "print(f\"\\n--- SIMULATION RESULTS ---\")\n",
    "print(f\"Particles 'Captured' by the P9 Warp Field: {captured}/{n_particles}\")\n",
    "\n",
    "# 4. Plot Evolution\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot Starting Positions (Grey)\n",
    "start_incs = inclinations[0]\n",
    "start_nodes = nodes[0]\n",
    "plt.scatter(start_nodes, start_incs, c='gray', alpha=0.3, label='Start (Random)')\n",
    "\n",
    "# Plot Final Positions (Blue)\n",
    "\n",
    "plt.scatter(final_nodes, final_incs, c='blue', alpha=0.6, label=f'End ({int(times[-1])} yrs)')\n",
    "\n",
    "# Plot Your Warp Target\n",
    "plt.scatter(TARGET_NODE, TARGET_INC, c='red', marker='X', s=200, label='REAL DATA Warp Center')\n",
    "\n",
    "# Draw Arrows showing movement\n",
    "for k in range(n_particles):\n",
    "    # connecting line\n",
    "    plt.plot([start_nodes[k], final_nodes[k]], [start_incs[k], final_incs[k]], 'k-', alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"Longitude of Ascending Node (deg)\")\n",
    "plt.ylabel(\"Inclination (deg)\")\n",
    "plt.title(f\"N-Body Test: Does P9 create the observed Warp?\\n(Target: i={TARGET_INC}, Node={TARGET_NODE})\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e4c41",
   "metadata": {},
   "source": [
    "üåÄ Phase 4: The Warp & \"Planet Y\"\n",
    "Script 21: The Warp Stress Test (GMM)\n",
    "The Logic:Phases 1-3 focused on angles ($\\varpi$). Phase 4 shifts to Inclination ($i$). The \"Planet Y\" hypothesis predicts that a closer, lighter planet would twist the mean plane of the Kuiper Belt (a \"Warp\").Why this exists:We use Gaussian Mixture Models (GMM), an unsupervised learning technique, to scan the inclination distribution. We are asking the AI: \"Are these objects all part of one flat disk, or are there two distinct planes titled relative to each other?\" Finding two distinct planes (bimodality in $i$-space) is the \"Smoking Gun\" for an inclined perturber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fb0d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "\n",
    "# 1. THE IRON FILTER (Requested by Critique)\n",
    "# They want stable ETNOs: a > 250, q > 40\n",
    "df_iron = df[ (df['a'] > 250) & (df['q'] > 40) ].copy()\n",
    "print(f\"Applying Iron Filter (a>250, q>40)...\")\n",
    "print(f\"Original Count: {len(df)}\")\n",
    "print(f\"Stable Survivors: {len(df_iron)}\")\n",
    "\n",
    "if len(df_iron) < 10:\n",
    "    print(\"WARNING: Sample size too small for clustering. Proceeding with caution.\")\n",
    "\n",
    "# 2. PREPARE WARP SPACE\n",
    "# We cluster in (Inclination, Node_Sin, Node_Cos)\n",
    "X = pd.DataFrame({\n",
    "    'i': df_iron['i'],\n",
    "    'node_sin': np.sin(np.radians(df_iron['Node'])),\n",
    "    'node_cos': np.cos(np.radians(df_iron['Node']))\n",
    "})\n",
    "\n",
    "# Scale features (Critical for GMM)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 3. GAUSSIAN MIXTURE MODEL (GMM)\n",
    "# We test 1 to 5 potential planes (components) and pick the best fit using BIC\n",
    "# BIC (Bayesian Information Criterion) penalizes overfitting.\n",
    "lowest_bic = np.inf\n",
    "best_gmm = None\n",
    "best_n = 0\n",
    "\n",
    "print(\"\\n--- GMM MODEL SELECTION ---\")\n",
    "for n in range(1, 6):\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42, n_init=10)\n",
    "    gmm.fit(X_scaled)\n",
    "    bic = gmm.bic(X_scaled)\n",
    "    print(f\"Components: {n} | BIC: {bic:.1f}\")\n",
    "    if bic < lowest_bic:\n",
    "        lowest_bic = bic\n",
    "        best_gmm = gmm\n",
    "        best_n = n\n",
    "\n",
    "print(f\"\\nWinner: {best_n} Orbital Populations Detected.\")\n",
    "\n",
    "# 4. ANALYZE THE WARP\n",
    "# Predict which plane each object belongs to\n",
    "labels = best_gmm.predict(X_scaled)\n",
    "probs = best_gmm.predict_proba(X_scaled)\n",
    "df_iron['cluster'] = labels\n",
    "df_iron['warp_prob'] = probs.max(axis=1) # Confidence\n",
    "\n",
    "# Get parameters of the main cluster\n",
    "# We unscale the means to get physical degrees\n",
    "# (This is approximate as we scaled inputs, simpler to re-calc mean of labeled data)\n",
    "print(\"\\n--- DETECTED PLANES (Iron Filter) ---\")\n",
    "for c in range(best_n):\n",
    "    cluster_data = df_iron[df_iron['cluster'] == c]\n",
    "    count = len(cluster_data)\n",
    "    mean_i = cluster_data['i'].mean()\n",
    "    \n",
    "    # Calculate mean angle for Node properly (vector mean)\n",
    "    s = np.mean(np.sin(np.radians(cluster_data['Node'])))\n",
    "    c_val = np.mean(np.cos(np.radians(cluster_data['Node'])))\n",
    "    mean_node = np.degrees(np.arctan2(s, c_val)) % 360\n",
    "    \n",
    "    print(f\"Plane {c+1}: N={count} | Inclination={mean_i:.1f} deg | Node={mean_node:.1f} deg\")\n",
    "\n",
    "# 5. VISUALIZATION\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot by Cluster\n",
    "colors = ['red', 'blue', 'green', 'purple', 'orange']\n",
    "for c in range(best_n):\n",
    "    subset = df_iron[df_iron['cluster'] == c]\n",
    "    plt.scatter(subset['Node'], subset['i'], c=colors[c], s=100, alpha=0.7, \n",
    "                edgecolors='black', label=f'Plane {c+1} (i={subset[\"i\"].mean():.1f})')\n",
    "\n",
    "plt.xlabel(\"Longitude of Ascending Node (deg)\")\n",
    "plt.ylabel(\"Inclination (deg)\")\n",
    "plt.title(f\"Robustness Check: GMM Clustering on Stable ETNOs\\n(a > 250 AU, q > 40 AU, N={len(df_iron)})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6533ce",
   "metadata": {},
   "source": [
    "Script 22: Model Face-Off (Planet 9 vs. Planet Y)\n",
    "The Logic:We now have two competing theories:\n",
    "\n",
    "Planet 9: Distant (500 AU), Heavy (5-10 $M_{\\oplus}$), aligns orbits via secular shepherding.\n",
    "\n",
    "Planet Y: Closer (100-150 AU), Lighter (Earth-mass), creates a \"Warp\" in inclination.\n",
    "\n",
    "Why this exists:This script sets up side-by-side N-body simulations to see which model better reproduces the current smart_dataset.csv. It is a direct \"Tournament\" to see which set of physics parameters best matches reality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d906b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rebound\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We run two parallel simulations\n",
    "# 1. P9 Model (The Heavyweight)\n",
    "P9_PARAMS = {'m': 5e-5, 'a': 500, 'e': 0.25, 'inc': 20, 'omega': 114} # Node aligned w/ Plane 2\n",
    "\n",
    "# 2. Planet Y Model (The Lightweight - based on Critique/Plane 1)\n",
    "PY_PARAMS = {'m': 3e-6, 'a': 150, 'e': 0.1, 'inc': 11, 'omega': 212} # Node aligned w/ Plane 1 ~212\n",
    "\n",
    "print(\"--- MODEL FACE-OFF: PLANET 9 VS PLANET Y ---\")\n",
    "\n",
    "def run_simulation(planet_params, label):\n",
    "    print(f\"Running {label} Simulation...\")\n",
    "    sim = rebound.Simulation()\n",
    "    sim.add(m=1.0) # Sun\n",
    "    # Add Giant Planets (Approx secular influence)\n",
    "    sim.add(m=0.00095, a=5.2)\n",
    "    sim.add(m=0.00028, a=9.5)\n",
    "    sim.add(m=0.00004, a=19.2)\n",
    "    sim.add(m=0.00005, a=30.0)\n",
    "    \n",
    "    # Add The Candidate\n",
    "    sim.add(m=planet_params['m'], a=planet_params['a'], e=planet_params['e'], \n",
    "            inc=np.radians(planet_params['inc']), Omega=np.radians(planet_params['omega']))\n",
    "    \n",
    "    # Add Test Particles (Stable Zone: a > 250)\n",
    "    final_nodes = []\n",
    "    final_incs = []\n",
    "    \n",
    "    # Inject 50 particles\n",
    "    for _ in range(50):\n",
    "        sim.add(a=np.random.uniform(250, 400), e=np.random.uniform(0.1, 0.4), \n",
    "                inc=np.radians(np.random.uniform(0, 30)), Omega=np.random.uniform(0, 2*np.pi))\n",
    "        \n",
    "    sim.move_to_com()\n",
    "    # Run for 250,000 years (Short secular check)\n",
    "    sim.integrate(250000)\n",
    "    \n",
    "    for p in sim.particles[6:]:\n",
    "        final_nodes.append(np.degrees(p.Omega) % 360)\n",
    "        final_incs.append(np.degrees(p.inc))\n",
    "        \n",
    "    return final_nodes, final_incs\n",
    "\n",
    "# Run Both\n",
    "p9_nodes, p9_incs = run_simulation(P9_PARAMS, \"Planet 9\")\n",
    "py_nodes, py_incs = run_simulation(PY_PARAMS, \"Planet Y\")\n",
    "\n",
    "# --- PLOT THE RESULTS ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Real Data Targets\n",
    "plt.scatter(114.2, 17.9, s=200, c='red', marker='X', label='Real Plane 2 (P9 Candidate)')\n",
    "plt.scatter(212.3, 11.2, s=200, c='green', marker='X', label='Real Plane 1 (Planet Y Candidate)')\n",
    "\n",
    "# Plot P9 Results\n",
    "plt.scatter(p9_nodes, p9_incs, c='red', alpha=0.3, label='P9 Simulation Particles')\n",
    "\n",
    "# Plot PY Results\n",
    "plt.scatter(py_nodes, py_incs, c='green', alpha=0.3, label='Planet Y Sim Particles')\n",
    "\n",
    "plt.xlabel(\"Longitude of Node (deg)\")\n",
    "plt.ylabel(\"Inclination (deg)\")\n",
    "plt.title(\"Dynamical Forensics: Which Planet Matches the Data?\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ce800",
   "metadata": {},
   "source": [
    "Script 23: Secular Analytic Solver\n",
    "The Logic: Instead of slow N-body simulations, this script uses Lagrange-Laplace Secular Theory. It solves the torque balance equations analytically. Why this exists: To quickly compare \"Planet 9\" vs \"Planet Y\". We generate theoretical curves for both planets and overlay the Real Data. We can instantly see which curve fits the observed warp better without running supercomputers for weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30111feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# REAL DATA TARGETS (From your GMM Analysis)\n",
    "REAL_P9_WARP = {'i': 17.9, 'node': 114.2}\n",
    "REAL_PY_WARP = {'i': 11.2, 'node': 212.3}\n",
    "\n",
    "print(\"--- ANALYTIC SECULAR SOLVER ---\")\n",
    "\n",
    "def calc_forced_plane(planet_m, planet_a, planet_i, planet_node, particle_a):\n",
    "    \"\"\"\n",
    "    Calculates the 'Forced Inclination' and 'Forced Node' imposed by a distant planet.\n",
    "    Based on linear secular theory (Lagrange-Laplace).\n",
    "    \"\"\"\n",
    "    # Constants\n",
    "    m_sun = 1.0\n",
    "    \n",
    "    # The 'Strength' of the planet's perturbation (coefficient B)\n",
    "    # Proportional to mass / a^3\n",
    "    # For a particle inside the planet's orbit (a < a_p)\n",
    "    # Approx: B ~ (m_p / m_sun) * (a / a_p^2) * b_laplace\n",
    "    \n",
    "    # Simplified Secular Equilibrium (The \"Fixed Point\"):\n",
    "    # If a particle is dominated by the planet, its orbit aligns with the planet's orbit.\n",
    "    # The 'Forced Plane' is essentially the Planet's Orbital Plane.\n",
    "    \n",
    "    # However, the Giant Planets (J/S/U/N) try to keep it at i=0.\n",
    "    # The result is a weighted average between i=0 (Giants) and i=Planet (P9).\n",
    "    \n",
    "    # Torque from Giants (J2 moment approx) vs Torque from P9\n",
    "    # Torque_Giants ~ 1 / particle_a^(7/2)\n",
    "    # Torque_P9 ~ particle_a^2 / planet_a^3\n",
    "    \n",
    "    # As particle_a increases, P9 wins.\n",
    "    # We calculate the 'Forced Inclination' (i_forced) at the particle's distance.\n",
    "    \n",
    "    # This is a toy model approximation of the Linear Secular solution\n",
    "    # i_forced = (Torque_P9 * i_P9) / (Torque_Giants + Torque_P9)\n",
    "    \n",
    "    torque_giants = 1.0 / (particle_a**3.5) * 1e8 # Scaling factor for inner system rigidity\n",
    "    torque_p9 = (planet_m / m_sun) * (particle_a**2) / (planet_a**3) * 1e10\n",
    "    \n",
    "    i_forced = (torque_p9 * planet_i) / (torque_giants + torque_p9)\n",
    "    \n",
    "    # The Node aligns with the dominant torquer (P9)\n",
    "    # If P9 dominates, Node -> Planet_Node\n",
    "    # If Giants dominate, Node precession is uniform (no lock)\n",
    "    # We model Node alignment as a transition\n",
    "    node_forced = planet_node if torque_p9 > torque_giants else np.random.uniform(0, 360)\n",
    "    \n",
    "    return i_forced, node_forced\n",
    "\n",
    "# --- TEST 1: PLANET 9 MODEL ---\n",
    "# 5 Earth Mass, 500 AU, i=20, Node=114\n",
    "print(\"\\nTesting Planet 9 Model...\")\n",
    "p9_i_results = []\n",
    "test_distances = np.linspace(150, 500, 50)\n",
    "\n",
    "for a in test_distances:\n",
    "    i_f, n_f = calc_forced_plane(5e-5, 500, 20, 114, a)\n",
    "    p9_i_results.append(i_f)\n",
    "\n",
    "# --- TEST 2: PLANET Y MODEL ---\n",
    "# 1 Earth Mass, 150 AU, i=10, Node=212\n",
    "print(\"Testing Planet Y Model...\")\n",
    "py_i_results = []\n",
    "for a in test_distances:\n",
    "    i_f, n_f = calc_forced_plane(3e-6, 150, 10, 212, a)\n",
    "    py_i_results.append(i_f)\n",
    "\n",
    "# --- PLOT COMPARISON ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Real Data Targets (Horizontal Lines)\n",
    "plt.axhline(REAL_P9_WARP['i'], color='red', linestyle='--', label=f'Real Warp A (i={REAL_P9_WARP[\"i\"]})')\n",
    "plt.axhline(REAL_PY_WARP['i'], color='green', linestyle='--', label=f'Real Warp B (i={REAL_PY_WARP[\"i\"]})')\n",
    "\n",
    "# Plot Theoretical Curves\n",
    "plt.plot(test_distances, p9_i_results, 'r-', linewidth=3, label='Theoretical P9 Influence')\n",
    "plt.plot(test_distances, py_i_results, 'g-', linewidth=3, label='Theoretical Planet Y Influence')\n",
    "\n",
    "plt.xlabel(\"Distance from Sun (AU)\")\n",
    "plt.ylabel(\"Forced Inclination (deg)\")\n",
    "plt.title(\"Analytic Check: Which Planet Creates the Observed Warp?\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b8eb83",
   "metadata": {},
   "source": [
    "Script 24: Bayesian Bootstrap Verification\n",
    "The Logic:Our sample size of \"Extreme\" objects is small ($N < 20$). In small datasets, a single outlier can fake a signal.\n",
    "\n",
    "Why this exists:We use Bayesian Bootstrapping to resample our dataset 2,000 times, creating 2,000 \"alternative realities\" based on our data. We run the Warp Detection (GMM) on every single one.\n",
    "\n",
    "The Metric: If 95% of the bootstrapped samples show the Warp, we have $2\\sigma$ confidence. If only 40% show it, the Warp is statistically insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48babfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Apply the \"Iron Filter\" (Stable objects only)\n",
    "df_stable = df[ (df['a'] > 250) & (df['q'] > 40) ].copy()\n",
    "print(f\"Starting Bayesian Bootstrap on {len(df_stable)} stable objects...\")\n",
    "\n",
    "# Prepare Data\n",
    "X = pd.DataFrame({\n",
    "    'i': df_stable['i'],\n",
    "    'node_sin': np.sin(np.radians(df_stable['Node'])),\n",
    "    'node_cos': np.cos(np.radians(df_stable['Node']))\n",
    "})\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# --- THE BAYESIAN BOOTSTRAP ---\n",
    "n_iterations = 2000 # Number of \"Parallel Universes\" to simulate\n",
    "warp_detections = []\n",
    "\n",
    "print(f\"Running {n_iterations} Resampling Trials...\")\n",
    "\n",
    "for k in range(n_iterations):\n",
    "    # 1. Resample Data (Bootstrap)\n",
    "    # We pick N objects from the dataset with replacement.\n",
    "    # This simulates \"what if we had slightly different data?\"\n",
    "    X_resampled = X.sample(n=len(X), replace=True, random_state=k)\n",
    "    X_res_scaled = scaler.transform(X_resampled) # Use original scaler\n",
    "    \n",
    "    # 2. Run GMM Clustering\n",
    "    # We look for the \"Best Fit\" number of planes (1 to 3)\n",
    "    best_bic = np.inf\n",
    "    best_model = None\n",
    "    \n",
    "    for n_comp in range(1, 4): # Check 1, 2, or 3 planes\n",
    "        gmm = GaussianMixture(n_components=n_comp, random_state=k)\n",
    "        gmm.fit(X_res_scaled)\n",
    "        bic = gmm.bic(X_res_scaled)\n",
    "        if bic < best_bic:\n",
    "            best_bic = bic\n",
    "            best_model = gmm\n",
    "    \n",
    "    # 3. Analyze the Winner\n",
    "    # Does it have a \"Warp\" component? (Cluster with i > 10 and i < 30)\n",
    "    means = best_model.means_\n",
    "    # Unscale the means to get rough inclination\n",
    "    # Note: Inverse transform is tricky for just one column, so we approximate\n",
    "    # We know 'i' is column 0.\n",
    "    # real_i = scaled_i * std + mean\n",
    "    mean_incs = means[:, 0] * scaler.scale_[0] + scaler.mean_[0]\n",
    "    \n",
    "    # Check if ANY cluster is a \"P9 Warp\" (between 12 and 22 degrees)\n",
    "    has_warp = np.any((mean_incs > 12) & (mean_incs < 22))\n",
    "    warp_detections.append(has_warp)\n",
    "\n",
    "    if k % 200 == 0:\n",
    "        print(f\"Trial {k}: {'Warp Found' if has_warp else 'Noise'}\")\n",
    "\n",
    "# --- FINAL STATISTICS ---\n",
    "confidence = np.mean(warp_detections) * 100\n",
    "print(f\"\\n--- FINAL BAYESIAN CONFIDENCE ---\")\n",
    "print(f\"Probability that the 17.9 deg Warp is REAL: {confidence:.1f}%\")\n",
    "\n",
    "# --- PLOT CONFIDENCE ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['Real Warp', 'Random Noise'], [confidence, 100-confidence], color=['green', 'gray'])\n",
    "plt.ylabel(\"Probability (%)\")\n",
    "plt.title(f\"Statistical Robustness of Warp Detection\\n(Confidence: {confidence:.1f}%)\")\n",
    "plt.ylim(0, 100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5d4745",
   "metadata": {},
   "source": [
    "Script 25: The Warp Inverter\n",
    "\n",
    "The Logic:Usually, we guess a planet's location and check the effect. This script does the Inverse Problem: it takes the observed magnitude of the Warp (e.g., $15^\\circ$ tilt) and mathematically solves the secular torque equations backward.\n",
    "\n",
    "Why this exists:Instead of guessing, this gives us a specific constraint: \"To tilt the Kuiper Belt by $15^\\circ$ against the pull of Jupiter/Saturn, a planet MUST be located on this specific curve of Mass vs. Distance.\" It narrows the search radius significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ef874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# 1. ISOLATE THE 9 \"WARP\" OBJECTS\n",
    "# We apply the Iron Filter (a>250, q>40)\n",
    "# And we filter for the specific Warp Plane (i approx 15-20, Node approx 100-130)\n",
    "# based on your previous GMM results (Plane 2).\n",
    "df_warp = df[ \n",
    "    (df['a'] > 250) & \n",
    "    (df['q'] > 40) & \n",
    "    (df['i'] > 14) & (df['i'] < 22) # The broad \"Warp Zone\"\n",
    "].copy()\n",
    "\n",
    "print(f\"Isolating Warp Candidates...\")\n",
    "print(f\"Found {len(df_warp)} objects fitting the profile.\")\n",
    "\n",
    "if len(df_warp) < 4:\n",
    "    print(\"Not enough points to fit a curve. Aborting.\")\n",
    "    exit()\n",
    "\n",
    "# 2. DEFINE THE PHYSICS MODEL (The \"Inverse\" Function)\n",
    "# We want to solve for 'planet_inc' and 'planet_mass'\n",
    "# x = particle_distance (a)\n",
    "# y = particle_inclination (i)\n",
    "\n",
    "def secular_model(particle_a, planet_inc, planet_mass_earth):\n",
    "    # Constants\n",
    "    m_sun = 1.0\n",
    "    planet_a = 500.0 # We fix distance at 500 AU (hard to constrain both mass/dist)\n",
    "    planet_m = planet_mass_earth * 3e-6 # Convert Earth Mass to Solar Mass\n",
    "    \n",
    "    # Torque Balance Formula (Same as Script 23)\n",
    "    # Torque Giants ~ 1/a^3.5\n",
    "    torque_giants = 1.0 / (particle_a**3.5) * 1e8 \n",
    "    # Torque P9 ~ a^2 / planet_a^3\n",
    "    torque_p9 = (planet_m / m_sun) * (particle_a**2) / (planet_a**3) * 1e10\n",
    "    \n",
    "    # Equilibrium Inclination\n",
    "    i_forced = (torque_p9 * planet_inc) / (torque_giants + torque_p9)\n",
    "    return i_forced\n",
    "\n",
    "# 3. RUN THE OPTIMIZER (Curve Fit)\n",
    "# We provide initial guesses: i=20, Mass=5\n",
    "# The code will adjust these to best fit the Real Data.\n",
    "p0 = [20.0, 5.0] \n",
    "# Bounds: Inc [0, 90], Mass [0.1, 20]\n",
    "bounds = ([0, 0.1], [90, 20])\n",
    "\n",
    "popt, pcov = curve_fit(secular_model, df_warp['a'], df_warp['i'], p0=p0, bounds=bounds)\n",
    "\n",
    "best_inc = popt[0]\n",
    "best_mass = popt[1]\n",
    "\n",
    "print(f\"\\n--- INVERSION RESULTS ---\")\n",
    "print(f\"The data implies Planet 9 has:\")\n",
    "print(f\"  Inclination: {best_inc:.2f} degrees\")\n",
    "print(f\"  Mass:        {best_mass:.2f} Earth Masses\")\n",
    "\n",
    "# 4. PLOT THE FIT\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Real Data\n",
    "plt.scatter(df_warp['a'], df_warp['i'], color='red', s=100, label='Real Objects (Stable)')\n",
    "\n",
    "# Plot the Optimized Curve\n",
    "x_range = np.linspace(200, 600, 100)\n",
    "y_pred = secular_model(x_range, best_inc, best_mass)\n",
    "plt.plot(x_range, y_pred, 'k--', linewidth=2, label=f'Best Fit Model (i={best_inc:.1f})')\n",
    "\n",
    "plt.axhline(best_inc, color='blue', linestyle=':', label='Predicted P9 Plane')\n",
    "\n",
    "plt.xlabel(\"Semi-Major Axis (a) [AU]\")\n",
    "plt.ylabel(\"Inclination (deg)\")\n",
    "plt.title(f\"Warp Inverter: Determining P9 Parameters from Data\\n(N={len(df_warp)})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee4abe0",
   "metadata": {},
   "source": [
    "Script 26: Pole Mapper\n",
    "The Logic: An orbit can be defined by its \"Pole\" (the vector perpendicular to the ring). If orbits share a plane, their poles should cluster in a tight spot on the sky. Why this exists: A visual \"sanity check.\" We plot the poles of all our objects. If they are scattered everywhere, the Warp is fake. If they bunch up like a target, the Warp is physical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09942d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"live_mpc_data.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 11 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "# Apply the Iron Filter (Stable only)\n",
    "df_stable = df[ (df['a'] > 250) & (df['q'] > 40) ].copy()\n",
    "\n",
    "print(f\"Mapping Orbital Poles of {len(df_stable)} stable objects...\")\n",
    "\n",
    "# 1. CALCULATE ORBITAL POLES\n",
    "# The pole is a vector perpendicular to the orbit.\n",
    "# In Cartesian coords (x,y,z):\n",
    "# Lx = sin(i) * sin(Node)\n",
    "# Ly = -sin(i) * cos(Node)\n",
    "# Lz = cos(i)\n",
    "# (Note: This points to the orbital \"North\")\n",
    "\n",
    "def get_pole_coords(row):\n",
    "    i_rad = np.radians(row['i'])\n",
    "    node_rad = np.radians(row['Node'])\n",
    "    \n",
    "    # Vector components\n",
    "    lx = np.sin(i_rad) * np.sin(node_rad)\n",
    "    ly = -np.sin(i_rad) * np.cos(node_rad)\n",
    "    lz = np.cos(i_rad)\n",
    "    \n",
    "    # Convert Vector -> RA/Dec on the Sky\n",
    "    # This tells us where the \"Axle\" of the orbit points\n",
    "    dec = np.degrees(np.arcsin(lz))\n",
    "    ra = np.degrees(np.arctan2(ly, lx)) % 360\n",
    "    \n",
    "    return pd.Series([ra, dec])\n",
    "\n",
    "df_stable[['Pole_RA', 'Pole_Dec']] = df_stable.apply(get_pole_coords, axis=1)\n",
    "\n",
    "# 2. THE VISUAL TEST\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the Poles\n",
    "# If random: Scattered everywhere.\n",
    "# If Warp: Clustered in one tight spot.\n",
    "plt.scatter(df_stable['Pole_RA'], df_stable['Pole_Dec'], c='blue', s=100, edgecolors='black', label='Orbital Poles')\n",
    "\n",
    "# Plot the \"Average Pole\" of your Warp Candidate\n",
    "# Warp was approx i=17.9, Node=114\n",
    "warp_i = np.radians(17.9)\n",
    "warp_node = np.radians(114)\n",
    "w_lx = np.sin(warp_i) * np.sin(warp_node)\n",
    "w_ly = -np.sin(warp_i) * np.cos(warp_node)\n",
    "w_lz = np.cos(warp_i)\n",
    "w_dec = np.degrees(np.arcsin(w_lz))\n",
    "w_ra = np.degrees(np.arctan2(w_ly, w_lx)) % 360\n",
    "\n",
    "plt.scatter(w_ra, w_dec, c='red', marker='X', s=300, label='Predicted Warp Center')\n",
    "\n",
    "plt.xlabel(\"Pole Right Ascension (deg)\")\n",
    "plt.ylabel(\"Pole Declination (deg)\")\n",
    "plt.title(f\"The Naked Eye Test: Do the Orbital Poles Cluster?\\n(N={len(df_stable)})\")\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(-90, 90)\n",
    "\n",
    "# Add \"Target Zones\"\n",
    "# If points are here, they support the Warp\n",
    "circle = plt.Circle((w_ra, w_dec), 10, color='red', fill=False, linestyle='--', label='10-deg Confidence Zone')\n",
    "plt.gca().add_patch(circle)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379fd8aa",
   "metadata": {},
   "source": [
    "Script 27: JPL Smart Harvester\n",
    "\n",
    "The Logic:The MPC data is good, but NASA JPL data is better (contains more precise arc-lengths and uncertainty parameters). This script introduces the Tisserand Parameter ($T_N$) as a physics-based filter.\n",
    "\n",
    "Why this exists:To filter out \"Scattered Disk Objects\" that are just bouncing off Neptune. We only want \"Detached\" objects ($T_N > 3$) because those are the only ones that carry the clean fossil record of Planet 9's gravity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc676ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SAVE_FILE = \"smart_dataset.csv\"\n",
    "JPL_API_URL = \"https://ssd-api.jpl.nasa.gov/sbdb_query.api\"\n",
    "\n",
    "print(\"--- INITIATING SMART HARVEST (Tisserand Filter) ---\")\n",
    "\n",
    "# 1. Query NASA JPL for TNOs [cite: 128]\n",
    "# We fetch full orbital elements including 'tp' (time of perihelion) for completeness\n",
    "params = {\n",
    "    \"sb-class\": \"TNO\",\n",
    "    \"fields\": \"full_name,a,e,i,w,om,q,tp\", \n",
    "    \"full-prec\": \"true\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"Contacting JPL Servers...\")\n",
    "    response = requests.get(JPL_API_URL, params=params)\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    \n",
    "    cols = data['fields']\n",
    "    raw_rows = data['data']\n",
    "    \n",
    "    df = pd.DataFrame(raw_rows, columns=cols)\n",
    "    \n",
    "    # Clean Data: Convert strings to numeric\n",
    "    for c in ['a', 'e', 'i', 'w', 'om', 'q']:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "    \n",
    "    df = df.rename(columns={'om': 'Node', 'full_name': 'ID'})\n",
    "    \n",
    "    # 2. CALCULATE TISSERAND PARAMETER (T_N)\n",
    "    # T_N > 3 implies stability (detached from Neptune scattering) [cite: 116]\n",
    "    # Formula: T_N = a_N/a + 2 * sqrt( (a/a_N) * (1-e^2) ) * cos(i)\n",
    "    a_Neptune = 30.07\n",
    "    \n",
    "    df['T_N'] = (a_Neptune / df['a']) + 2 * np.sqrt( (df['a'] / a_Neptune) * (1 - df['e']**2) ) * np.cos(np.radians(df['i']))\n",
    "    \n",
    "    # 3. APPLY SMART FILTERS\n",
    "    # Filter A: Distance (Relaxed to > 150 AU to capture transition objects) [cite: 129]\n",
    "    mask_dist = (df['a'] > 150)\n",
    "    \n",
    "    # Filter B: Stability (Physics-based)\n",
    "    # We set q > 38 AU to ensure objects are outside the immediate scattering zone of Neptune (a_N=30)\n",
    "    # This aligns with the critique's suggestion to avoid \"overfiltering\" while excluding transients.\n",
    "    mask_stable = (df['q'] > 38)\n",
    "    \n",
    "    df_smart = df[mask_dist & mask_stable].copy()\n",
    "    \n",
    "    print(f\"Total TNOs Downloaded: {len(df)}\")\n",
    "    print(f\"Smart Filter Survivors: {len(df_smart)}\")\n",
    "    print(f\"  (Criteria: a > 150 AU AND q > 38 AU)\")\n",
    "    \n",
    "    # Save to CSV for subsequent GMM analysis\n",
    "    df_smart.to_csv(SAVE_FILE, index=False)\n",
    "    print(f\"Saved to {SAVE_FILE}\")\n",
    "    \n",
    "    # Check against the critique's target of N > 30 [cite: 129]\n",
    "    if len(df_smart) > 30:\n",
    "        print(\"\\nSUCCESS: Dataset size > 30. Critique Requirement Satisfied.\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: Dataset size is still small. Further relaxation (a > 120) may be needed.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Harvest Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dcd4ec",
   "metadata": {},
   "source": [
    "Script 28: Expanded Warp Test (Bootstrap)\n",
    "The Logic: We re-run the Warp Detection (GMM) on the new, clean \"Smart Dataset.\" We add Bayesian Bootstrapping: we resample the dataset 1,000 times to see how often the Warp appears. Why this exists: To prove robustness. If the Warp disappears when we remove 1 or 2 objects, it's not real. If it survives 95% of the bootstrap trials, it's a statistically significant structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30e7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"smart_dataset.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 27 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Loading Smart Dataset: {len(df)} Objects\")\n",
    "\n",
    "# 1. PREPARE DATA (Inclination & Node)\n",
    "# We use the same phase space as before\n",
    "X = pd.DataFrame({\n",
    "    'i': df['i'],\n",
    "    'node_sin': np.sin(np.radians(df['Node'])),\n",
    "    'node_cos': np.cos(np.radians(df['Node']))\n",
    "})\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. FIND BEST FIT MODEL (GMM)\n",
    "# We test 1-5 components and pick the best BIC\n",
    "best_bic = np.inf\n",
    "best_gmm = None\n",
    "best_n = 0\n",
    "\n",
    "print(\"\\n--- CLUSTERING ANALYSIS ---\")\n",
    "for n in range(1, 6):\n",
    "    gmm = GaussianMixture(n_components=n, random_state=42, n_init=10)\n",
    "    gmm.fit(X_scaled)\n",
    "    bic = gmm.bic(X_scaled)\n",
    "    if bic < best_bic:\n",
    "        best_bic = bic\n",
    "        best_gmm = gmm\n",
    "        best_n = n\n",
    "\n",
    "print(f\"Optimal Model Complexity: {best_n} Planes Detected\")\n",
    "\n",
    "# 3. EXTRACT WARP PARAMETERS\n",
    "# Predict labels\n",
    "labels = best_gmm.predict(X_scaled)\n",
    "df['cluster'] = labels\n",
    "\n",
    "# Analyze clusters\n",
    "found_warp = False\n",
    "warp_params = {}\n",
    "\n",
    "print(\"\\n--- DETECTED STRUCTURES ---\")\n",
    "for c in range(best_n):\n",
    "    subset = df[df['cluster'] == c]\n",
    "    count = len(subset)\n",
    "    mean_i = subset['i'].mean()\n",
    "    \n",
    "    # Vector mean for Node\n",
    "    s = np.mean(np.sin(np.radians(subset['Node'])))\n",
    "    c_val = np.mean(np.cos(np.radians(subset['Node'])))\n",
    "    mean_node = np.degrees(np.arctan2(s, c_val)) % 360\n",
    "    \n",
    "    print(f\"Cluster {c+1}: N={count} | Inclination={mean_i:.1f} deg | Node={mean_node:.1f} deg\")\n",
    "    \n",
    "    # Check if this matches our \"Warp Profile\" (i between 12 and 25)\n",
    "    if 12 < mean_i < 25:\n",
    "        found_warp = True\n",
    "        warp_params = {'i': mean_i, 'node': mean_node, 'n': count}\n",
    "        print(f\"  >>> MATCHES PLANET 9/Y PROFILE <<<\")\n",
    "\n",
    "# 4. BAYESIAN BOOTSTRAP (The Robustness Check)\n",
    "# We run 1000 trials to see how stable this detection is\n",
    "if found_warp:\n",
    "    print(f\"\\n--- STRESS TESTING WARP ({warp_params['i']:.1f} deg) ---\")\n",
    "    trials = 1000\n",
    "    successes = 0\n",
    "    \n",
    "    for k in range(trials):\n",
    "        # Resample with replacement\n",
    "        X_res = X.sample(n=len(X), replace=True, random_state=k)\n",
    "        X_res_s = scaler.transform(X_res)\n",
    "        \n",
    "        # Fit GMM (Fixed to best_n components for stability)\n",
    "        gmm_b = GaussianMixture(n_components=best_n, random_state=k)\n",
    "        gmm_b.fit(X_res_s)\n",
    "        \n",
    "        # Check means\n",
    "        means = gmm_b.means_\n",
    "        # Unscale Inclination (approx)\n",
    "        # i is col 0\n",
    "        real_i = means[:, 0] * scaler.scale_[0] + scaler.mean_[0]\n",
    "        \n",
    "        # Is there a cluster in the 12-25 deg range?\n",
    "        if np.any((real_i > 12) & (real_i < 25)):\n",
    "            successes += 1\n",
    "            \n",
    "    confidence = (successes / trials) * 100\n",
    "    print(f\"Bootstrap Confidence (N={len(df)}): {confidence:.1f}%\")\n",
    "else:\n",
    "    print(\"\\nNo Warp Candidate found in this dataset.\")\n",
    "\n",
    "# 5. POLE MAP (Visual Check)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Poles\n",
    "# Pole definition:\n",
    "# Lx = sin(i)sin(Node), Ly = -sin(i)cos(Node), Lz = cos(i)\n",
    "# RA = atan2(Ly, Lx), Dec = asin(Lz)\n",
    "i_rad = np.radians(df['i'])\n",
    "n_rad = np.radians(df['Node'])\n",
    "lx = np.sin(i_rad) * np.sin(n_rad)\n",
    "ly = -np.sin(i_rad) * np.cos(n_rad)\n",
    "lz = np.cos(i_rad)\n",
    "pole_dec = np.degrees(np.arcsin(lz))\n",
    "pole_ra = np.degrees(np.arctan2(ly, lx)) % 360\n",
    "\n",
    "\n",
    "plt.scatter(pole_ra, pole_dec, c=labels, cmap='viridis', s=80, edgecolors='black', label='Objects')\n",
    "\n",
    "if found_warp:\n",
    "    # Plot the Center of the Warp\n",
    "    w_i = np.radians(warp_params['i'])\n",
    "    w_n = np.radians(warp_params['node'])\n",
    "    w_lx = np.sin(w_i) * np.sin(w_n)\n",
    "    w_ly = -np.sin(w_i) * np.cos(w_n)\n",
    "    w_lz = np.cos(w_i)\n",
    "    w_dec = np.degrees(np.arcsin(w_lz))\n",
    "    w_ra = np.degrees(np.arctan2(w_ly, w_lx)) % 360\n",
    "    plt.scatter(w_ra, w_dec, c='red', marker='X', s=200, label=f'Warp Center ({warp_params[\"i\"]:.1f} deg)')\n",
    "\n",
    "plt.xlabel(\"Pole RA (deg)\")\n",
    "plt.ylabel(\"Pole Dec (deg)\")\n",
    "plt.title(f\"New Dataset Validation (N={len(df)})\\nDoes the Warp Persist?\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(0, 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b2e1e0",
   "metadata": {},
   "source": [
    "Script 29: Naked Eye HistogramThe Logic:Sometimes complex AI confuses things. This script makes a simple Histogram and KDE (Kernel Density Estimate) of the inclinations.Why this exists:The \"Grandmother Test.\" If you can't see the bump in the histogram with your naked eye, the statistical tests might be lying. We look for a distinct peak at ~$16^\\circ$ separate from the main Kuiper Belt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558727e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"smart_dataset.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 27 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"Analyzing Distribution of {len(df)} Objects...\")\n",
    "\n",
    "# 1. SETUP HISTOGRAM DATA\n",
    "# We are looking strictly at Inclination (i)\n",
    "incs = df['i']\n",
    "\n",
    "# 2. CALCULATE KERNEL DENSITY ESTIMATION (KDE)\n",
    "# This creates a smooth curve representing the probability distribution\n",
    "# Bandwidth method 'scott' or 'silverman' determines smoothness\n",
    "density = gaussian_kde(incs)\n",
    "x_vals = np.linspace(0, 60, 200)\n",
    "density_vals = density(x_vals)\n",
    "\n",
    "# Find the Peak of the Curve (The \"Most Likely\" Warp Angle)\n",
    "peak_idx = np.argmax(density_vals)\n",
    "peak_inc = x_vals[peak_idx]\n",
    "\n",
    "print(f\"\\n--- DISTRIBUTION ANALYSIS ---\")\n",
    "print(f\"Primary Peak Detected at: {peak_inc:.1f} degrees\")\n",
    "\n",
    "# Check for secondary peaks (simple method)\n",
    "# Find local maxima\n",
    "from scipy.signal import find_peaks\n",
    "peaks, _ = find_peaks(density_vals, height=0.01)\n",
    "peak_angles = x_vals[peaks]\n",
    "\n",
    "print(f\"All Density Spikes: {peak_angles}\")\n",
    "\n",
    "# 3. PLOT THE \"NAKED EYE\" TEST\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram (The raw counts)\n",
    "plt.hist(incs, bins=range(0, 60, 3), density=True, alpha=0.3, color='gray', label='Raw Counts (Bins)')\n",
    "\n",
    "# KDE Curve (The smooth reality)\n",
    "plt.plot(x_vals, density_vals, 'b-', linewidth=3, label='Probability Density (KDE)')\n",
    "\n",
    "# Mark the Peaks\n",
    "for p in peak_angles:\n",
    "    plt.axvline(p, color='red', linestyle='--', alpha=0.6)\n",
    "    plt.text(p, max(density_vals)*1.02, f\"{p:.1f}¬∞\", color='red', ha='center', fontweight='bold')\n",
    "\n",
    "# Reference Lines (The Competing Theories)\n",
    "plt.axvline(18.0, color='green', linestyle=':', alpha=0.5, label='Theory: Planet 9 (~18¬∞)')\n",
    "plt.axvline(10.0, color='orange', linestyle=':', alpha=0.5, label='Theory: Planet Y (~10¬∞)')\n",
    "\n",
    "plt.xlabel(\"Inclination (degrees)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(f\"The 'Naked Eye' Test: Is the Warp Visible?\\n(N={len(df)} Stable Objects)\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 50)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc542fe",
   "metadata": {},
   "source": [
    "Script 30: Final Sky TrackThe Logic:This generates the final \"Publication Quality\" map. It uses the refined Warp parameters from Script 29 ($i=15.7^\\circ$) to draw the ultimate search line for the planet.Why this exists:To output the final coordinates for telescope operators. It highlights \"Prime Observation Windows\" where the planet track is far from the Galactic Plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f2fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# THE FINAL CONFIRMED PARAMETERS (From Script 29)\n",
    "FINAL_INC = 15.7   # The \"Primary Peak\"\n",
    "FINAL_NODE = 114.0 # We stick with the Node from the \"High Warp\" group (Plane 2) which was the strongest signal\n",
    "\n",
    "print(f\"--- GENERATING FINAL PUBLICATION TRACK ---\")\n",
    "print(f\"Targeting Orbital Plane: i={FINAL_INC}, Node={FINAL_NODE}\")\n",
    "\n",
    "def get_orbit_track(inc_deg, node_deg):\n",
    "    inc = np.radians(inc_deg)\n",
    "    node = np.radians(node_deg)\n",
    "    v = np.linspace(0, 2*np.pi, 500)\n",
    "    \n",
    "    # Orbital Frame -> Ecliptic Frame\n",
    "    x_ecl = np.cos(node)*np.cos(v) - np.sin(node)*np.sin(v)*np.cos(inc)\n",
    "    y_ecl = np.sin(node)*np.cos(v) + np.cos(node)*np.sin(v)*np.cos(inc)\n",
    "    z_ecl = np.sin(v)*np.sin(inc)\n",
    "    \n",
    "    # Ecliptic -> Equatorial (RA/Dec)\n",
    "    epsilon = np.radians(23.439)\n",
    "    x_eq = x_ecl\n",
    "    y_eq = y_ecl * np.cos(epsilon) - z_ecl * np.sin(epsilon)\n",
    "    z_eq = y_ecl * np.sin(epsilon) + z_ecl * np.cos(epsilon)\n",
    "    \n",
    "    ra = np.degrees(np.arctan2(y_eq, x_eq)) % 360\n",
    "    dec = np.degrees(np.arcsin(z_eq))\n",
    "    \n",
    "    return pd.DataFrame({'RA': ra, 'Dec': dec}).sort_values('RA')\n",
    "\n",
    "p9_track = get_orbit_track(FINAL_INC, FINAL_NODE)\n",
    "gal_track = get_orbit_track(62.87, 282.85) # Galactic Plane\n",
    "\n",
    "# --- PLOT FOR PUBLICATION ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "# 1. The Galaxy (Avoidance Zone)\n",
    "plt.scatter(gal_track['RA'], gal_track['Dec'], c='gray', s=10, alpha=0.3, label='Galactic Plane (High Stellar Density)')\n",
    "\n",
    "# 2. The Final Search Track\n",
    "plt.plot(p9_track['RA'], p9_track['Dec'], 'r-', linewidth=3, label=f'Planet Y Search Track (i={FINAL_INC}¬∞)')\n",
    "\n",
    "# 3. Mark the \"Best Bet\" Zones\n",
    "# These are regions where the track is highest in the sky (best visibility) and far from the galaxy\n",
    "plt.axvspan(30, 90, color='green', alpha=0.1, label='Prime Observation Window (Fall/Winter)')\n",
    "plt.axvspan(210, 270, color='green', alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"Right Ascension (deg)\")\n",
    "plt.ylabel(\"Declination (deg)\")\n",
    "plt.title(f\"Targeting Map for Earth-Mass Perturber\\n(Based on Secular Warp at i={FINAL_INC}¬∞)\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 360)\n",
    "plt.ylim(-60, 60)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# --- OUTPUT COORDINATES TABLE ---\n",
    "print(\"\\n--- TELESCOPE TARGET LIST (Top 5 Vectors) ---\")\n",
    "# Sample points every 45 degrees of RA for quick reference\n",
    "targets = p9_track.iloc[::50] # Downsample\n",
    "print(targets[['RA', 'Dec']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924efdc1",
   "metadata": {},
   "source": [
    "Script 31: Unbiased PCA Decomposition\n",
    "The Logic: Humans are biased‚Äîwe like looking for clusters in variables we understand (like Argument of Perihelion). Principal Component Analysis (PCA) is a dimensionality reduction technique that finds the \"axes of maximum variance\" without human instruction. \n",
    "\n",
    "Why this exists: This acts as a \"Double Blind\" test. We feed the raw orbital elements into PCA. If the First Principal Component (PC1) naturally separates the objects into the same clusters we found manually, it proves the structure is inherent to the data, not a result of us \"massaging\" the specific variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4911d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"smart_dataset.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 27 first.\")\n",
    "    exit()\n",
    "\n",
    "df = pd.read_csv(INPUT_FILE)\n",
    "print(f\"--- UNBIASED DECOMPOSITION (N={len(df)}) ---\")\n",
    "\n",
    "# 1. PREPARE FEATURE SPACE (Full Orbital Elements)\n",
    "# Critique requested: a, e, i, Omega (Node), omega (Peri) [cite: 121]\n",
    "# We convert angles to sin/cos components to avoid the 0/360 discontinuity\n",
    "features = pd.DataFrame({\n",
    "    'a_norm': df['a'], # We will scale this\n",
    "    'e': df['e'],\n",
    "    'i': df['i'],\n",
    "    'node_sin': np.sin(np.radians(df['Node'])),\n",
    "    'node_cos': np.cos(np.radians(df['Node'])),\n",
    "    'peri_sin': np.sin(np.radians(df['w'])),\n",
    "    'peri_cos': np.cos(np.radians(df['w']))\n",
    "})\n",
    "\n",
    "# 2. STANDARDIZATION (Critical for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# 3. PRINCIPAL COMPONENT ANALYSIS (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(X_scaled)\n",
    "df_pca = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Analyze Variance\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"\\nVariance Explained by PC1: {explained_variance[0]*100:.1f}%\")\n",
    "print(f\"Variance Explained by PC2: {explained_variance[1]*100:.1f}%\")\n",
    "\n",
    "# Analyze Loadings (What is PC1 made of?)\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2'], index=features.columns)\n",
    "print(\"\\n--- PC1 DRIVERS (Correlations) ---\")\n",
    "print(loadings['PC1'].sort_values(ascending=False))\n",
    "# If 'i' or 'node' are high, the Warp is the dominant feature of the solar system.\n",
    "\n",
    "# 4. HIERARCHICAL CLUSTERING (Agglomerative) \n",
    "# We test 2 to 5 clusters to find the best Silhouette Score\n",
    "best_score = -1\n",
    "best_k = 0\n",
    "best_labels = None\n",
    "\n",
    "print(\"\\n--- HIERARCHICAL CLUSTERING ---\")\n",
    "for k in range(2, 6):\n",
    "    clusterer = AgglomerativeClustering(n_clusters=k)\n",
    "    labels = clusterer.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    print(f\"Clusters: {k} | Silhouette Score: {score:.3f}\")\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "        best_labels = labels\n",
    "\n",
    "print(f\"\\nWinner: {best_k} Clusters with Score {best_score:.3f}\")\n",
    "\n",
    "# 5. VALIDATE THE CRITIQUE'S THRESHOLD\n",
    "if best_score > 0.5:\n",
    "    print(\">>> VALIDITY CHECK PASSED (Score > 0.5) <<<\")\n",
    "    print(\"The structure is mathematically real and distinct.\")\n",
    "else:\n",
    "    print(\">>> VALIDITY CHECK FAILED (Score < 0.5) <<<\")\n",
    "    print(\"The critique is right: The data is likely random noise.\")\n",
    "\n",
    "# 6. VISUALIZATION\n",
    "df['cluster'] = best_labels\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(x=df_pca['PC1'], y=df_pca['PC2'], hue=df['cluster'], palette='viridis', s=100, edgecolor='black')\n",
    "\n",
    "# Annotate with feature vectors (Biplot style)\n",
    "# This shows us physically what PC1 and PC2 mean\n",
    "for i, feature in enumerate(features.columns):\n",
    "    # Scale vectors for visibility\n",
    "    plt.arrow(0, 0, pca.components_[0, i]*3, pca.components_[1, i]*3, color='r', alpha=0.5)\n",
    "    plt.text(pca.components_[0, i]*3.2, pca.components_[1, i]*3.2, feature, color='r')\n",
    "\n",
    "plt.xlabel(f\"PC1 ({explained_variance[0]*100:.1f}%)\")\n",
    "plt.ylabel(f\"PC2 ({explained_variance[1]*100:.1f}%)\")\n",
    "plt.title(f\"PCA Decomposition: The Underlying Geometry of the Outer Solar System\\n(N={len(df)}, Silhouette={best_score:.3f})\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(0, color='black', linewidth=1)\n",
    "plt.axvline(0, color='black', linewidth=1)\n",
    "plt.show()\n",
    "\n",
    "# 7. CHARACTERIZE THE GROUPS\n",
    "print(\"\\n--- GROUP PROFILES ---\")\n",
    "for c in range(best_k):\n",
    "    subset = df[df['cluster'] == c]\n",
    "    print(f\"Group {c}: N={len(subset)} | Mean Inc={subset['i'].mean():.1f} | Mean Node={subset['Node'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36cfda",
   "metadata": {},
   "source": [
    "Script 32: The Bias Emulator (Null Universe)\n",
    "The Logic: This script creates a \"Null Universe\"‚Äîa synthetic solar system where Planet 9 does not exist (randomly distributed orbits). It then applies a \"Virtual Telescope Filter\" that mimics the exact pointing history and limitations of real Earth-based surveys. \n",
    "\n",
    "Why this exists: This is the ultimate sanity check. If our \"Null Universe\" + \"Survey Bias\" looks exactly like our real data, then Planet 9 is dead‚Äîour \"discovery\" was just bias. We need to prove that the Real Data is statistically distinct from the Biased Null Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca814a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import anderson_ksamp\n",
    "import os\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "INPUT_FILE = \"smart_dataset.csv\"\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"Error: Run Script 27 first.\")\n",
    "    exit()\n",
    "\n",
    "df_real = pd.read_csv(INPUT_FILE)\n",
    "real_incs = df_real['i'].values\n",
    "\n",
    "print(f\"--- BIAS EMULATOR: TESTING NULL HYPOTHESIS ---\")\n",
    "print(f\"Real Data: N={len(real_incs)} objects\")\n",
    "\n",
    "# 1. GENERATE THE \"NULL UNIVERSE\" (Flat Solar System)\n",
    "# We assume a standard \"scattered disk\" distribution:\n",
    "# Inclinations follow a sin(i)*Gaussian distribution (Brown 2001)\n",
    "# Null Hypothesis: The underlying population is \"Standard\" (peak ~10 deg, broad)\n",
    "# NOT Warped at 16 deg.\n",
    "n_sim = 10000\n",
    "print(f\"Simulating {n_sim} synthetic objects (Null Hypothesis)...\")\n",
    "\n",
    "# Generate standard scattered disk inclinations (Brown 2001, sigma=10)\n",
    "sigma_i = 10.0\n",
    "raw_incs = np.random.rayleigh(sigma_i, n_sim) \n",
    "# Note: Rayleigh matches the sin(i)*gaussian physics of a hot disk\n",
    "\n",
    "# Generate random sky positions (RA/Dec) for these objects\n",
    "# An object at Inclination 'i' spends most time at max declination +/- i\n",
    "# We approximate Dec ~ i * sin(random_phase)\n",
    "phase = np.random.uniform(0, 2*np.pi, n_sim)\n",
    "raw_decs = raw_incs * np.sin(phase)\n",
    "raw_ras = np.random.uniform(0, 360, n_sim)\n",
    "\n",
    "# 2. APPLY SURVEY BIAS (The \"Window\")\n",
    "# We filter these fake objects through the \"Survey Mask\"\n",
    "# Approximate footprints of major surveys (OSSOS, DES, etc.)\n",
    "# Real surveys heavily favor the Ecliptic (Dec +/- 10)\n",
    "# But they have \"blocks\" off-ecliptic.\n",
    "\n",
    "def is_in_survey(ra, dec):\n",
    "    # Ecliptic Survey (The \"Band\")\n",
    "    if abs(dec) < 5: return True\n",
    "    \n",
    "    # OSSOS-like Blocks (Approximate)\n",
    "    if (10 < ra < 50) and (abs(dec) < 15): return True\n",
    "    if (300 < ra < 350) and (abs(dec) < 15): return True\n",
    "    \n",
    "    # High-Latitude checks (Very rare)\n",
    "    # Most surveys MISS high-i objects because they look at the ecliptic.\n",
    "    return False\n",
    "\n",
    "# Filter the Null Universe\n",
    "visible_indices = [k for k in range(n_sim) if is_in_survey(raw_ras[k], raw_decs[k])]\n",
    "biased_incs = raw_incs[visible_indices]\n",
    "\n",
    "print(f\"Simulated Survey Detection Rate: {len(biased_incs)/n_sim*100:.1f}%\")\n",
    "\n",
    "# 3. ANDERSON-DARLING TEST\n",
    "# Critique Requirement: \"Reject warp if p > 0.05\"\n",
    "# We compare the \"Real\" distribution to the \"Biased Null\" distribution\n",
    "# If they are DIFFERENT, then Bias cannot explain your data.\n",
    "\n",
    "statistic, critical_values, significance_level = anderson_ksamp([real_incs, biased_incs])\n",
    "\n",
    "print(\"\\n--- STATISTICAL VERDICT (Anderson-Darling) ---\")\n",
    "print(f\"Statistic: {statistic:.4f}\")\n",
    "print(f\"Significance Levels: {significance_level}\")\n",
    "print(f\"Critical Values:     {critical_values}\")\n",
    "\n",
    "# Interpretation logic for k-sample AD test\n",
    "# If statistic > critical_value at 5%, we reject the null (They are different).\n",
    "is_different = statistic > critical_values[2] # Index 2 is usually 5% level\n",
    "\n",
    "if is_different:\n",
    "    print(\"\\n>>> RESULT: REJECT NULL HYPOTHESIS (p < 0.05)\")\n",
    "    print(\"Survey bias ALONE cannot explain the observed warp.\")\n",
    "    print(\"The 15.7 deg structure is statistically distinct from a biased flat disk.\")\n",
    "else:\n",
    "    print(\"\\n>>> RESULT: CANNOT REJECT NULL (p > 0.05)\")\n",
    "    print(\"WARNING: The 'Warp' looks just like a biased selection of normal objects.\")\n",
    "    print(\"Critique was right: This might be an observational illusion.\")\n",
    "\n",
    "# 4. PLOT COMPARISON\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Real Data (KDE)\n",
    "from scipy.stats import gaussian_kde\n",
    "x_eval = np.linspace(0, 50, 200)\n",
    "kde_real = gaussian_kde(real_incs)\n",
    "plt.plot(x_eval, kde_real(x_eval), 'r-', linewidth=3, label='Real Data (Warped?)')\n",
    "\n",
    "# Biased Null (KDE)\n",
    "kde_null = gaussian_kde(biased_incs)\n",
    "plt.plot(x_eval, kde_null(x_eval), 'k--', linewidth=2, label='Biased Null Model (Standard Disk)')\n",
    "\n",
    "plt.axvline(15.7, color='red', alpha=0.3, label='Detected Warp (15.7 deg)')\n",
    "\n",
    "plt.xlabel(\"Inclination (deg)\")\n",
    "plt.ylabel(\"Probability Density\")\n",
    "plt.title(f\"Bias Quantification: Real Data vs. Survey-Biased Null\\n(AD Statistic={statistic:.2f})\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14958511",
   "metadata": {},
   "source": [
    "Script 33: High-Inclination Solver ($i=65^\\circ$)\n",
    "\n",
    "The Logic:Critiques suggested the planet might be highly inclined ($i=65^\\circ$). This script modifies the physics engine to solve for a planet at this specific steep angle.\n",
    "\n",
    "Why this exists:To check the \"Planet Y\" hypothesis specifically. Can a 1-Earth-mass planet at $65^\\circ$ inclination create the warp we see? This script solves for the exact distance required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c067c0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_WARP = 15.7      # The observed angle (from Script 29)\n",
    "PLANET_INC = 65.0       # The hypothesized rigid inclination\n",
    "PARTICLE_DIST = 400.0   # Average distance of our ETNOs\n",
    "\n",
    "print(f\"--- HIGH-INCLINATION SOLVER (DEBUG MODE) ---\")\n",
    "print(f\"Goal: Create a {TARGET_WARP}¬∞ Warp using a {PLANET_INC}¬∞ Planet.\")\n",
    "\n",
    "# 1. DEFINE THE PHYSICS\n",
    "# We use the same 'Tune' from Script 25 that gave us 1.65 Me @ 500 AU = 17 deg\n",
    "# Calibration Factor derived from Script 25 result:\n",
    "# Torque_ratio = (1.65 * 3e-6 * 400**2 / 500**3 * 1e10) / (1/400**3.5 * 1e8) approx 1.0\n",
    "# We stick to the relative formula to ensure consistency.\n",
    "\n",
    "def get_required_mass(planet_dist, target_warp, planet_inc):\n",
    "    # Torque Giant Planets (Force trying to flatten orbit)\n",
    "    torque_giants = 1.0 / (PARTICLE_DIST**3.5) * 1e8\n",
    "    \n",
    "    # Torque Planet X (Force trying to lift orbit)\n",
    "    # We solve for Torque_P in the balance equation:\n",
    "    # Warp = (Torque_P * Inc_P) / (Torque_G + Torque_P)\n",
    "    # Warp * Torque_G = Torque_P * (Inc_P - Warp)\n",
    "    # Torque_P = (Warp * Torque_G) / (Inc_P - Warp)\n",
    "    \n",
    "    if target_warp >= planet_inc: return np.inf\n",
    "    \n",
    "    required_torque_p = (target_warp * torque_giants) / (planet_inc - target_warp)\n",
    "    \n",
    "    # Convert Torque -> Mass\n",
    "    # Torque_P = (Mass_Solar) * (Part_a^2) / (Planet_a^3) * 1e10\n",
    "    # Mass_Solar = Torque_P * Planet_a^3 / (Part_a^2 * 1e10)\n",
    "    \n",
    "    mass_solar = required_torque_p * (planet_dist**3) / (PARTICLE_DIST**2) / 1e10\n",
    "    mass_earth = mass_solar / 3e-6 # Convert to Earth Masses\n",
    "    return mass_earth\n",
    "\n",
    "# 2. CALCULATE CURVE\n",
    "dist_range = np.linspace(100, 2000, 500) # Scan 100 to 2000 AU\n",
    "mass_curve = [get_required_mass(d, TARGET_WARP, PLANET_INC) for d in dist_range]\n",
    "\n",
    "# Check bounds to avoid the \"Straight Line\" error\n",
    "print(f\"Min Mass Found: {min(mass_curve):.3f} Earth Masses\")\n",
    "print(f\"Max Mass Found: {max(mass_curve):.3f} Earth Masses\")\n",
    "\n",
    "# 3. FIND EXACT SOLUTIONS (Interpolation)\n",
    "# We create a function d(m) to find distance for a given mass\n",
    "dist_from_mass = interp1d(mass_curve, dist_range, bounds_error=False, fill_value=\"extrapolate\")\n",
    "\n",
    "sol_1M = float(dist_from_mass(1.0))\n",
    "sol_5M = float(dist_from_mass(5.0))\n",
    "\n",
    "print(\"\\n--- SOLUTIONS ---\")\n",
    "if 100 < sol_1M < 2000:\n",
    "    print(f\"1. A 1.0 Earth-Mass Planet (Planet Y) must be at: {sol_1M:.0f} AU\")\n",
    "else:\n",
    "    print(f\"1. A 1.0 Earth-Mass Planet is impossible in this range (Need >2000 AU).\")\n",
    "\n",
    "if 100 < sol_5M < 2000:\n",
    "    print(f\"2. A 5.0 Earth-Mass Planet (Planet 9) must be at: {sol_5M:.0f} AU\")\n",
    "else:\n",
    "    print(f\"2. A 5.0 Earth-Mass Planet is impossible in this range (Need >2000 AU).\")\n",
    "\n",
    "# 4. PLOT\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(dist_range, mass_curve, 'b-', linewidth=2, label=f'Required Mass for {TARGET_WARP}¬∞ Warp')\n",
    "\n",
    "# Mark solutions\n",
    "if 100 < sol_1M < 2000:\n",
    "    plt.scatter([sol_1M], [1.0], color='green', s=100, zorder=5, label='Planet Y Solution')\n",
    "    plt.axvline(sol_1M, color='green', linestyle=':', alpha=0.5)\n",
    "\n",
    "if 100 < sol_5M < 2000:\n",
    "    plt.scatter([sol_5M], [5.0], color='red', s=100, zorder=5, label='Planet 9 Solution')\n",
    "    plt.axvline(sol_5M, color='red', linestyle=':', alpha=0.5)\n",
    "\n",
    "plt.axhline(1.0, color='green', alpha=0.1)\n",
    "plt.axhline(5.0, color='red', alpha=0.1)\n",
    "\n",
    "plt.xlabel(\"Planet Distance (AU)\")\n",
    "plt.ylabel(\"Required Mass (Earth Masses)\")\n",
    "plt.title(f\"Trade-Off: If Planet is at {PLANET_INC}¬∞, where must it be?\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.yscale('log') # Log scale is crucial here\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2eec60",
   "metadata": {},
   "source": [
    "Script 34: Planet Y Warp Check\n",
    "The Logic: We verify the specific candidates proposed in scientific literature (e.g., a Mars-mass planet at 80 AU). Why this exists: To \"Rule In\" or \"Rule Out\" existing theories. We calculate exactly how much warp those specific planets would cause and see if it matches our 15.7-degree measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ac07ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Team Giants (Mass in Earth Masses, Distance in AU)\n",
    "giants = [\n",
    "    {'name': 'Jupiter', 'm': 317.8, 'a': 5.2},\n",
    "    {'name': 'Saturn',  'm': 95.2,  'a': 9.5},\n",
    "    {'name': 'Uranus',  'm': 14.5,  'a': 19.2},\n",
    "    {'name': 'Neptune', 'm': 17.1,  'a': 30.1}\n",
    "]\n",
    "\n",
    "# Team Planet Y Candidates (Literature values)\n",
    "candidates = [\n",
    "    {'name': 'Planet Y (Lower)', 'm': 1.0, 'a': 60, 'inc': 65},\n",
    "    {'name': 'Planet Y (Upper)', 'm': 1.0, 'a': 80, 'inc': 65},\n",
    "    {'name': 'Super-Earth Y',    'm': 2.5, 'a': 100, 'inc': 65} # Some papers suggest up to 3Me\n",
    "]\n",
    "\n",
    "print(\"--- PLANET Y PHYSICS CHECK (INNER PERTURBER MODEL) ---\")\n",
    "\n",
    "# 1. CALCULATE GIANT PLANET STRENGTH (J2 Moment)\n",
    "# Strength proportional to Mass * a^2\n",
    "strength_giants = 0\n",
    "for p in giants:\n",
    "    s = p['m'] * (p['a']**2)\n",
    "    strength_giants += s\n",
    "    # print(f\"{p['name']}: Strength = {s:.0f}\")\n",
    "\n",
    "print(f\"Total Strength of Known Solar System (J2): {strength_giants:.0f}\")\n",
    "\n",
    "# 2. TEST CANDIDATES\n",
    "print(\"\\n--- PREDICTED WARP ANGLES ---\")\n",
    "predicted_warps = []\n",
    "names = []\n",
    "\n",
    "for p in candidates:\n",
    "    strength_y = p['m'] * (p['a']**2)\n",
    "    \n",
    "    # The Weighted Average Formula\n",
    "    # Angle = (Strength_Y * Inc_Y + Strength_Giants * 0) / (Strength_Y + Strength_Giants)\n",
    "    warp_angle = (strength_y * p['inc']) / (strength_y + strength_giants)\n",
    "    \n",
    "    print(f\"Candidate: {p['name']} ({p['m']} Me @ {p['a']} AU)\")\n",
    "    print(f\"  > Strength: {strength_y:.0f}\")\n",
    "    print(f\"  > Predicted Warp: {warp_angle:.2f} degrees\")\n",
    "    \n",
    "    predicted_warps.append(warp_angle)\n",
    "    names.append(p['name'])\n",
    "\n",
    "# 3. VISUALIZE THE MATCH\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot Candidates\n",
    "plt.bar(names, predicted_warps, color=['green', 'blue', 'purple'], alpha=0.7)\n",
    "\n",
    "# Plot Your Data\n",
    "plt.axhline(15.7, color='red', linewidth=3, linestyle='--', label='Your Data (15.7¬∞)')\n",
    "plt.axhspan(14.0, 17.5, color='red', alpha=0.1, label='Error Margin')\n",
    "\n",
    "plt.ylabel(\"Predicted Warp Inclination (deg)\")\n",
    "plt.title(\"Does 'Planet Y' Explain Your Data?\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 30)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943b4b5",
   "metadata": {},
   "source": [
    "Script 35: The Planet Optimizer (Final Solver)\n",
    "The Logic: This script combines the findings of Script 25 (Inverter) and Script 21 (Warp) into a final physics solver. It calculates the torque balance between the Galactic Tide (which pulls orbits out) and the Giant Planets (which flatten them). \n",
    "\n",
    "Why this exists: This is the conclusion of the notebook. It answers the question: \"What specific Mass and Semi-major Axis are required to maintain the observed inclination warp against the flattening force of Jupiter and Saturn?\" The output is the final \"Target Zone\" for observers to hunt for the planet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953635f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_WARP = 15.7       # From Script 29\n",
    "ETNO_DIST = 400.0        # Average distance of our stable rocks\n",
    "GIANT_J2 = 38023.0       # Strength of Giants (Earth Mass * AU^2) from Script 34\n",
    "\n",
    "print(f\"--- DUAL-REGIME PLANET OPTIMIZER ---\")\n",
    "print(f\"Target: Explain the {TARGET_WARP}¬∞ Warp.\")\n",
    "\n",
    "# --- PHYSICS ENGINE 1: PLANET Y (INNER REGIME) ---\n",
    "# Formula: Weighted Average of Planes (J2 Balance)\n",
    "def solve_inner_mass(planet_dist, planet_inc):\n",
    "    if TARGET_WARP >= planet_inc: return np.inf\n",
    "    # warp = (Strength_Y * Inc_Y) / (Strength_G + Strength_Y)\n",
    "    # Strength_Y * (Inc_Y - warp) = warp * Strength_G\n",
    "    # Strength_Y = (warp * Strength_G) / (Inc_Y - warp)\n",
    "    # Strength_Y = Mass * dist^2\n",
    "    \n",
    "    req_strength = (TARGET_WARP * GIANT_J2) / (planet_inc - TARGET_WARP)\n",
    "    mass = req_strength / (planet_dist**2)\n",
    "    return mass\n",
    "\n",
    "# --- PHYSICS ENGINE 2: PLANET 9 (OUTER REGIME) ---\n",
    "# Formula: External Torque Balance (Laplace-Lagrange)\n",
    "def solve_outer_mass(planet_dist, planet_inc):\n",
    "    if TARGET_WARP >= planet_inc: return np.inf\n",
    "    \n",
    "    # Torque A (Giants trying to flatten): Proportional to 1/r^3.5\n",
    "    tau_giants = 1.0 / (ETNO_DIST**3.5) * 1e8\n",
    "    \n",
    "    # Torque B (Planet 9 trying to lift): Proportional to Mass * r^2 / P_dist^3\n",
    "    # Balance: Warp = (Tau_B * Inc_P) / (Tau_A + Tau_B)\n",
    "    # Tau_B = (Warp * Tau_A) / (Inc_P - Warp)\n",
    "    \n",
    "    req_tau_b = (TARGET_WARP * tau_giants) / (planet_inc - TARGET_WARP)\n",
    "    \n",
    "    # Invert Torque B to get Mass\n",
    "    # Tau_B = (Mass_Solar) * (ETNO_a^2) / (Planet_a^3) * 1e10\n",
    "    mass_solar = req_tau_b * (planet_dist**3) / (ETNO_DIST**2) / 1e10\n",
    "    mass_earth = mass_solar / 3e-6\n",
    "    return mass_earth\n",
    "\n",
    "# --- RUN THE SOLVERS ---\n",
    "\n",
    "# 1. OPTIMIZE PLANET Y (Fixed at 65 deg inclination)\n",
    "y_dists = np.linspace(60, 150, 100)\n",
    "y_masses = [solve_inner_mass(d, 65.0) for d in y_dists]\n",
    "\n",
    "# 2. OPTIMIZE PLANET 9 (Fixed at 65 deg inclination)\n",
    "p9_dists = np.linspace(300, 1200, 100)\n",
    "p9_masses = [solve_outer_mass(d, 65.0) for d in p9_dists]\n",
    "\n",
    "# --- FIND \"LITERATURE MATCHES\" ---\n",
    "# Planet Y is theorized to be Earth-Mass (~1.0). Where does that happen?\n",
    "y_solver = interp1d(y_masses, y_dists, fill_value=\"extrapolate\")\n",
    "optimal_y_dist = float(y_solver(1.0))\n",
    "\n",
    "# Planet 9 is theorized to be 5 Earth-Masses. Where does that happen?\n",
    "p9_solver = interp1d(p9_masses, p9_dists, fill_value=\"extrapolate\")\n",
    "optimal_p9_dist = float(p9_solver(5.0))\n",
    "\n",
    "print(\"\\n--- OPTIMIZATION RESULTS ---\")\n",
    "print(f\"THEORY Y (Inner Perturber, i=65¬∞):\")\n",
    "print(f\"  To match the data with 1.0 Earth Mass, it MUST be at: {optimal_y_dist:.1f} AU\")\n",
    "\n",
    "print(f\"\\nTHEORY 9 (Outer Shepherd, i=65¬∞):\")\n",
    "print(f\"  To match the data with 5.0 Earth Masses, it MUST be at: {optimal_p9_dist:.1f} AU\")\n",
    "\n",
    "# --- VISUALIZATION ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot Y\n",
    "ax1.plot(y_dists, y_masses, 'g-', linewidth=3)\n",
    "ax1.scatter([optimal_y_dist], [1.0], color='black', s=100, zorder=5)\n",
    "ax1.axvline(optimal_y_dist, color='green', linestyle=':', label=f'Optimal Dist: {optimal_y_dist:.0f} AU')\n",
    "ax1.set_title(\"Planet Y Optimization (i=65¬∞)\")\n",
    "ax1.set_xlabel(\"Distance (AU)\")\n",
    "ax1.set_ylabel(\"Required Mass (Earth Masses)\")\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 9\n",
    "ax2.plot(p9_dists, p9_masses, 'r-', linewidth=3)\n",
    "ax2.scatter([optimal_p9_dist], [5.0], color='black', s=100, zorder=5)\n",
    "ax2.axvline(optimal_p9_dist, color='red', linestyle=':', label=f'Optimal Dist: {optimal_p9_dist:.0f} AU')\n",
    "ax2.set_title(\"Planet 9 Optimization (i=65¬∞)\")\n",
    "ax2.set_xlabel(\"Distance (AU)\")\n",
    "ax2.set_ylabel(\"Required Mass (Earth Masses)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d43123",
   "metadata": {},
   "source": [
    "Script 36: The Final Showdown (Physics Comparative)\n",
    "\n",
    "The Logic:We have reached the end of the pipeline. We have confirmed the \"Warp\" ($i \\approx 15.7^\\circ$) and we have two competing suspects:\n",
    "\n",
    "Planet 9: The canonical theory (Heavy, Distant, Low Inclination $\\approx 20^\\circ$).\n",
    "Planet Y: The alternative theory (Light, Closer, High Inclination $\\approx 65^\\circ$).\n",
    "\n",
    "Why this exists:This script runs the physics engines for both theories simultaneously on the same graph. It answers the ultimate question: \"To produce the 15.7-degree warp we observe, where would each planet need to be?\"If the Planet 9 curve requires a mass of 50 Earths to work, the theory is dead.If the Planet Y curve lands exactly on 1 Earth Mass at 100-120 AU, it is the stronger candidate.The Verdict: The script outputs a direct plausibility score based on whether the required distance falls within known observational limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_WARP = 15.7       # The observed signal\n",
    "ETNO_DIST = 400.0        # Where the rocks are\n",
    "GIANT_J2 = 38023.0       # Strength of Jupiter/Saturn/etc (from Script 34)\n",
    "\n",
    "print(f\"--- FINAL SHOWDOWN: PLANET Y vs PLANET 9 ---\")\n",
    "\n",
    "# --- PHYSICS ENGINE 1: PLANET Y (Inner, i=65) ---\n",
    "def get_y_mass(d, inc=65.0):\n",
    "    if TARGET_WARP >= inc: return np.nan\n",
    "    # J2 Balance Model\n",
    "    req_strength = (TARGET_WARP * GIANT_J2) / (inc - TARGET_WARP)\n",
    "    mass = req_strength / (d**2)\n",
    "    return mass\n",
    "\n",
    "# --- PHYSICS ENGINE 2: PLANET 9 (Outer, i=20) ---\n",
    "def get_p9_mass(d, inc=20.0):\n",
    "    if TARGET_WARP >= inc: return np.nan\n",
    "    # External Torque Model\n",
    "    tau_giants = 1.0 / (ETNO_DIST**3.5) * 1e8\n",
    "    req_tau_b = (TARGET_WARP * tau_giants) / (inc - TARGET_WARP)\n",
    "    mass_solar = req_tau_b * (d**3) / (ETNO_DIST**2) / 1e10\n",
    "    mass_earth = mass_solar / 3e-6\n",
    "    return mass_earth\n",
    "\n",
    "# --- GENERATE CURVES ---\n",
    "# We scan 50 to 1500 AU to catch everything\n",
    "dists = np.linspace(50, 1500, 500)\n",
    "\n",
    "y_curve = [get_y_mass(d, 65.0) for d in dists]\n",
    "p9_curve = [get_p9_mass(d, 20.0) for d in dists]\n",
    "\n",
    "# --- FIND SOLUTIONS ---\n",
    "# 1. Planet Y (1.0 Me)\n",
    "try:\n",
    "    y_solver = interp1d(y_curve, dists, fill_value=\"extrapolate\")\n",
    "    opt_y = float(y_solver(1.0))\n",
    "except:\n",
    "    opt_y = 0\n",
    "\n",
    "# 2. Planet 9 (5.0 Me)\n",
    "try:\n",
    "    p9_solver = interp1d(p9_curve, dists, fill_value=\"extrapolate\")\n",
    "    opt_p9 = float(p9_solver(5.0))\n",
    "except:\n",
    "    opt_p9 = 0\n",
    "\n",
    "print(\"\\n--- THE VERDICT ---\")\n",
    "print(f\"PLANET Y SCENARIO (Target: 1 Me, Inc: 65¬∞)\")\n",
    "print(f\"  > Required Distance: {opt_y:.1f} AU\")\n",
    "print(f\"  > Plausibility: {'HIGH' if 80 < opt_y < 150 else 'LOW'}\")\n",
    "\n",
    "print(f\"\\nPLANET 9 SCENARIO (Target: 5 Me, Inc: 20¬∞)\")\n",
    "print(f\"  > Required Distance: {opt_p9:.1f} AU\")\n",
    "print(f\"  > Plausibility: {'HIGH' if 400 < opt_p9 < 800 else 'LOW'}\")\n",
    "\n",
    "# --- PLOT ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Y\n",
    "plt.plot(dists, y_curve, 'g-', linewidth=3, label='Planet Y Theory (i=65¬∞)')\n",
    "plt.scatter([opt_y], [1.0], color='green', s=150, zorder=10, edgecolors='black')\n",
    "plt.text(opt_y, 1.2, f\"  1 $M_E$ @ {opt_y:.0f} AU\", color='green', fontweight='bold')\n",
    "\n",
    "# Plot 9\n",
    "plt.plot(dists, p9_curve, 'r-', linewidth=3, label='Planet 9 Theory (i=20¬∞)')\n",
    "plt.scatter([opt_p9], [5.0], color='red', s=150, zorder=10, edgecolors='black')\n",
    "plt.text(opt_p9, 5.5, f\"  5 $M_E$ @ {opt_p9:.0f} AU\", color='red', fontweight='bold')\n",
    "\n",
    "# Zones\n",
    "plt.axhspan(0.8, 1.2, color='green', alpha=0.1, label='Earth-Mass Range')\n",
    "plt.axhspan(4.5, 5.5, color='red', alpha=0.1, label='Super-Earth Range')\n",
    "\n",
    "plt.title(f\"Which Planet Fits the {TARGET_WARP}¬∞ Warp Best?\")\n",
    "plt.xlabel(\"Distance (AU)\")\n",
    "plt.ylabel(\"Required Mass (Earth Masses)\")\n",
    "plt.yscale('log')\n",
    "plt.ylim(0.1, 50)\n",
    "plt.xlim(0, 1200)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
